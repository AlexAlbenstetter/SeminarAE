{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "from torch import optim\n",
    "from river import compose, metrics, preprocessing, stream, anomaly, linear_model\n",
    "from OnlineTorch.anomaly import TorchAE, SklearnAnomalyDetector\n",
    "from tqdm import tqdm\n",
    "import river  \n",
    "import torchvision\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import SGDOneClassSVM\n",
    "from util import build_anomaly_dataset, Tensor2Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_incremental_base(model, data, update_interv=100):\n",
    "    scores = []\n",
    "    truths = []\n",
    "    iterator = tqdm(data, unit='samples')\n",
    "    iterator.set_description('Learning from stream')\n",
    "    loss_sum = 0\n",
    "    idx = 0\n",
    "    metric = metrics.Accuracy()\n",
    "    for x, y in iterator:\n",
    "        #print(x)\n",
    "        score = model.predict_one(x)\n",
    "        model = model.learn_one(x,y)\n",
    "        scores.append(score)\n",
    "        if isinstance(y, torch.Tensor):\n",
    "            y = y.item()\n",
    "        truths.append(y)\n",
    "        loss_sum += score\n",
    "        idx += 1\n",
    "        if idx == update_interv:\n",
    "            iterator.set_postfix({f'loss_{update_interv}': loss_sum/update_interv})\n",
    "            loss_sum = 0\n",
    "            idx = 0\n",
    "        metric = metric.update(score,y)\n",
    "    #print(truths,scores)        \n",
    "    #print(metrics.Accuracy(truths,scores))\n",
    "    print(metric)\n",
    "    return roc_auc_score(truths, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_incremental(model, data, update_interv=100):\n",
    "    scores = []\n",
    "    truths = []\n",
    "    iterator = tqdm(data, unit='samples')\n",
    "    iterator.set_description('Learning from stream')\n",
    "    loss_sum = 0\n",
    "    idx = 0\n",
    "    for x, y in iterator:\n",
    "        model = model.learn_one(x)\n",
    "        score = model.score_one(x)\n",
    "        scores.append(score)\n",
    "        if isinstance(y, torch.Tensor):\n",
    "            y = y.item()\n",
    "        truths.append(y)\n",
    "        loss_sum += score\n",
    "        idx += 1\n",
    "        if idx == update_interv:\n",
    "            iterator.set_postfix({f'loss_{update_interv}': loss_sum/update_interv})\n",
    "            loss_sum = 0\n",
    "            idx = 0\n",
    "    return roc_auc_score(truths, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cae(n_features=1):\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=n_features, out_channels=32,\n",
    "                  kernel_size=3, stride=2),\n",
    "        nn.SELU(),\n",
    "        nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3, stride=2),\n",
    "        nn.SELU(),\n",
    "        nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3, stride=3),\n",
    "        nn.SELU(),\n",
    "        nn.ConvTranspose2d(in_channels=8, out_channels=16,\n",
    "                           kernel_size=3, stride=3),\n",
    "        nn.SELU(),\n",
    "        nn.ConvTranspose2d(in_channels=16, out_channels=32,\n",
    "                           kernel_size=3, stride=2),\n",
    "        nn.SELU(),\n",
    "        nn.ConvTranspose2d(in_channels=32, out_channels=n_features,\n",
    "                           kernel_size=4, stride=2),\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "loss = nn.L1Loss\n",
    "optimizer = optim.AdamW\n",
    "model = TorchAE(build_fn=build_cae, loss_fn=loss, device=device,\n",
    "                optimizer_fn=optimizer, learning_rate=0.01, seed=42)\n",
    "\n",
    "model2 = Tensor2Dict() | anomaly.HalfSpaceTrees(seed=20)\n",
    "model3 = SklearnAnomalyDetector(SGDOneClassSVM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manuel\\Anaconda3\\envs\\rwch\\lib\\site-packages\\torchvision\\datasets\\mnist.py:58: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n"
     ]
    }
   ],
   "source": [
    "mnist = torchvision.datasets.MNIST('./data/', download=True)\n",
    "mnist_x, mnist_y = mnist.train_data.unsqueeze(1) / 255., mnist.targets\n",
    "mnist = build_anomaly_dataset(mnist_x, mnist_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning from stream: 100%|██████████| 9631/9631 [01:07<00:00, 142.06samples/s, loss_100=0.0611]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9880754535021136"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_incremental(model=model, data=mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning from stream: 100%|██████████| 9631/9631 [00:58<00:00, 164.68samples/s, loss_100=0.557]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8960713049498096"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_incremental(model=model2, data=mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning from stream: 100%|██████████| 9631/9631 [00:06<00:00, 1516.81samples/s, loss_100=[14.7615074]] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5486082552720202"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_incremental(model=model3, data=mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cae_cifar(n_features=3):\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=n_features, out_channels=64,\n",
    "                  kernel_size=3, stride=2),\n",
    "        nn.SELU(),\n",
    "        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2),\n",
    "        nn.SELU(),\n",
    "        nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=2),\n",
    "        nn.SELU(),\n",
    "        nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2),\n",
    "        nn.SELU(),\n",
    "        nn.ConvTranspose2d(in_channels=256, out_channels=128,\n",
    "                           kernel_size=3, stride=2),\n",
    "        nn.SELU(),\n",
    "        nn.ConvTranspose2d(in_channels=128, out_channels=128,\n",
    "                           kernel_size=3, stride=2),\n",
    "        nn.SELU(),\n",
    "        nn.ConvTranspose2d(in_channels=128, out_channels=64,\n",
    "                           kernel_size=3, stride=2),\n",
    "        nn.SELU(),\n",
    "        nn.ConvTranspose2d(in_channels=64, out_channels=n_features,\n",
    "                           kernel_size=3, stride=2),\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning from stream: : 8000samples [00:22, 362.10samples/s, loss_100=0.048] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9816225705329154"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_ae(n_features, latent_dim=1):\n",
    "    model = nn.Sequential(\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(n_features, 20), \n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(20, latent_dim),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(latent_dim, 20),\n",
    "        nn.LeakyReLU(), \n",
    "        nn.Linear(20, n_features)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "loss =  nn.L1Loss\n",
    "optimizer = optim.AdamW\n",
    "model = compose.Pipeline(\n",
    "    preprocessing.MinMaxScaler(),\n",
    "    TorchAE(build_fn=build_ae, loss_fn=loss, optimizer_fn=optimizer, learning_rate=0.01, seed=42)\n",
    ")\n",
    "\n",
    "model2 = compose.Pipeline(\n",
    "    preprocessing.MinMaxScaler(),\n",
    "    anomaly.HalfSpaceTrees(seed=20)\n",
    ")\n",
    "\n",
    "model3= compose.Pipeline(\n",
    "    preprocessing.MinMaxScaler(),\n",
    "    linear_model.LogisticRegression())\n",
    "\n",
    "phishing = stream.shuffle(river.datasets.CreditCard().take(8000), 1000, seed=20)\n",
    "train_test_incremental(model, phishing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning from stream: : 40000samples [00:27, 1480.71samples/s, loss_100=0.378]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9336265877820796"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phishing = stream.shuffle(river.datasets.CreditCard().take(40000), 1000, seed=20)\n",
    "train_test_incremental(model2, phishing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning from stream: : 8000samples [00:02, 2738.79samples/s, loss_100=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phishing = stream.shuffle(river.datasets.CreditCard().take(8000), 1000, seed=20)\n",
    "train_test_incremental_base(model3, phishing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"phishing = stream.shuffle(river.datasets.CreditCard().take(8000), 1000, seed=20)\\nupdate_interv=100\\nscores = []\\ntruths = []\\niterator = tqdm(phishing, unit='samples')\\niterator.set_description('Learning from stream')\\nloss_sum = 0\\nidx = 0\\nfor x, y in iterator:\\n        #print(x)\\n        #model = model.learn_one(x,y)\\n        score = model3.predict_proba_one(x)\\n        print(score)\\n        scores.append(score)        \\n        truths.append(y)\\n        loss_sum += score\\n        idx += 1\\n        if idx == update_interv:\\n            iterator.set_postfix({f'loss_{update_interv}': loss_sum/update_interv})\\n            loss_sum = 0\\n            idx = 0\\n    #print(truths, scores)\\nprint(roc_auc_score(truths, scores))\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''phishing = stream.shuffle(river.datasets.CreditCard().take(8000), 1000, seed=20)\n",
    "update_interv=100\n",
    "scores = []\n",
    "truths = []\n",
    "iterator = tqdm(phishing, unit='samples')\n",
    "iterator.set_description('Learning from stream')\n",
    "loss_sum = 0\n",
    "idx = 0\n",
    "for x, y in iterator:\n",
    "        #print(x)\n",
    "        #model = model.learn_one(x,y)\n",
    "        score = model3.predict_proba_one(x)\n",
    "        print(score)\n",
    "        scores.append(score)        \n",
    "        truths.append(y)\n",
    "        loss_sum += score\n",
    "        idx += 1\n",
    "        if idx == update_interv:\n",
    "            iterator.set_postfix({f'loss_{update_interv}': loss_sum/update_interv})\n",
    "            loss_sum = 0\n",
    "            idx = 0\n",
    "    #print(truths, scores)\n",
    "print(roc_auc_score(truths, scores))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3= compose.Pipeline(\n",
    "    preprocessing.MinMaxScaler(),\n",
    "    linear_model.LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Amount': 149.62,\n",
      " 'Time': 0.0,\n",
      " 'V1': -1.3598071336738,\n",
      " 'V10': 0.0907941719789316,\n",
      " 'V11': -0.551599533260813,\n",
      " 'V12': -0.617800855762348,\n",
      " 'V13': -0.991389847235408,\n",
      " 'V14': -0.311169353699879,\n",
      " 'V15': 1.46817697209427,\n",
      " 'V16': -0.470400525259478,\n",
      " 'V17': 0.207971241929242,\n",
      " 'V18': 0.0257905801985591,\n",
      " 'V19': 0.403992960255733,\n",
      " 'V2': -0.0727811733098497,\n",
      " 'V20': 0.251412098239705,\n",
      " 'V21': -0.018306777944153,\n",
      " 'V22': 0.277837575558899,\n",
      " 'V23': -0.110473910188767,\n",
      " 'V24': 0.0669280749146731,\n",
      " 'V25': 0.128539358273528,\n",
      " 'V26': -0.189114843888824,\n",
      " 'V27': 0.133558376740387,\n",
      " 'V28': -0.0210530534538215,\n",
      " 'V3': 2.53634673796914,\n",
      " 'V4': 1.37815522427443,\n",
      " 'V5': -0.338320769942518,\n",
      " 'V6': 0.462387777762292,\n",
      " 'V7': 0.239598554061257,\n",
      " 'V8': 0.0986979012610507,\n",
      " 'V9': 0.363786969611213}\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "dataset = river.datasets.CreditCard()\n",
    "for x, y in dataset:\n",
    "    pprint(x)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object shuffle at 0x000002602ED305F0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phishing2 = stream.shuffle(river.datasets.Phishing().take(8000), 1000, seed=20)\n",
    "phishing2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250\n",
      "548\n",
      "702\n"
     ]
    }
   ],
   "source": [
    "phishing2 = stream.shuffle(river.datasets.Phishing().take(10000), 1000, seed=20)\n",
    "num_true=0\n",
    "num_false=0\n",
    "counter=0\n",
    "for x,y in phishing2:\n",
    "    counter+=1\n",
    "    if y == True:\n",
    "        num_true+=1\n",
    "    else:\n",
    "        num_false+=1\n",
    "print(counter)\n",
    "print(num_true)\n",
    "print(num_false)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = river.datasets.Phishing()\n",
    "counter = 0\n",
    "for x, y in dataset:\n",
    "    y = int(y == True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Learning from stream: : 1250samples [00:03, 402.12samples/s, loss_100=0.604]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6334170357892985"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = compose.Pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    TorchAE(build_fn=build_ae, loss_fn=loss, optimizer_fn=optimizer, learning_rate=0.01, seed=42)\n",
    ")\n",
    "phishing2 = stream.shuffle(dataset.take(8000), 1000, seed=20)\n",
    "train_test_incremental(model1, phishing2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e8b387b273876d5d45e7a8b26f076fc7482097c291cde7573e42877192c4357"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
