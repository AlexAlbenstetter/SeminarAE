{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "from torch import optim\n",
    "from river import compose, metrics, preprocessing, stream, anomaly, linear_model, datasets, compose\n",
    "from river import feature_extraction as fx\n",
    "from river.tree import HoeffdingAdaptiveTreeClassifier\n",
    "from river import optim as op\n",
    "\n",
    "from IncrementalTorch.anomaly.anomaly import Autoencoder, BasicAutoencoder\n",
    "from river import compat\n",
    "\n",
    "from tqdm import tqdm\n",
    "import river  \n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "from OnlineTorch.classifier import PyTorch2RiverClassifier\n",
    "from OnlineTorch.anomaly import TorchAE\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import SGDOneClassSVM\n",
    "from sklearn.cluster import k_means\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from river import evaluate\n",
    "\n",
    "\n",
    "N_SAMPLES = 5000 #number of elements stored in memory\n",
    "SEED = 42 #Random Seed for shuffles, constant\n",
    "#track_name = \"RBF\"\n",
    "#LOSS = nn.BCELoss\n",
    "LOSS = nn.L1Loss #Lossfunction, constant\n",
    "#LOSS = nn.CrossEntropyLoss\n",
    "OPTIMIZER = optim.AdamW #Optimizer, constant\n",
    "BATCH_SIZE=10 #Tracked for PCA, not necessary anymore\n",
    "LEARNING_RATE=1e-3 #Constant\n",
    "\n",
    "rocauc = river.metrics.ROCAUC() #metric for evaluation\n",
    "\n",
    "threshhold=0 #Tracked for Confusion Matrix, not necessary anymore\n",
    "\n",
    "#Latent Dim 1 or 2; both evaluated\n",
    "LATENT_DIM = 1\n",
    "\n",
    "##max 5% with 10000 samples\n",
    "anom_percentage_credit = [50,40,30,20,10,5,2.5,1.25,0.625,0.313,0.172] #everything evaluated\n",
    "num_samples_credit = 9840 #50% * 9840 --> fits with num anomalies\n",
    "\n",
    "anom_percentage_covtype = [50,40,30,20,10,5,2.5,1.25,0.625,0.313,0.172] #everything evaluated\n",
    "num_samples_covtype = 25000\n",
    "num_anom_classes_covtype=[1,2,3,4,5] #if you want 1 anom class type 1, up to 5 anomalie classes\n",
    "\n",
    "#Structure Result csv\n",
    "evaluation=pd.DataFrame(columns=['model','num_neurons','dataset','anom_percentage','num_samples','num_anom_classes','learn_supervised','Loss','Optimizer','Batch_size','Learning_rate','Latent_dim','ROC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covtype Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class 5 contains 9493 instances\n",
    "class 2 contains 283301 instances\n",
    "class 1 contains 211840 instances\n",
    "class 7 contains 20510 instances\n",
    "class 3 contains 35754 instances\n",
    "class 6 contains 17367 instances\n",
    "class 4 contains 2747 instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create dataset out of covertype dataset, working with random shuffles of all anomalie classes, take according number of anomalies, fitting dataset into stream format\n",
    "def make_covtype_dataset(anom_percentage_covtype,num_samples_covtype,num_anom_classes_covtype):  \n",
    "    covtype_x, covtype_y=fetch_covtype(as_frame=True,return_X_y=True)\n",
    "    covtype_df=covtype_x\n",
    "    covtype_df['target']=covtype_y\n",
    "    covtype_df['target']=covtype_df['target']-1\n",
    "    covtype_df_classes=covtype_df.target.unique()\n",
    "    for i in covtype_df_classes:\n",
    "        print('class {0} contains {1} instances'.format(i,covtype_df[covtype_df.target==i].Elevation.count()))\n",
    "\n",
    "    for i in range (1,num_anom_classes_covtype+1):\n",
    "        covtype_df.target[covtype_df.target==i]=1\n",
    "        covtype_df.loc[covtype_df['target']==i, 'target']=1\n",
    "\n",
    "    covtype_df_filtered=covtype_df[covtype_df.target<=num_anom_classes_covtype]\n",
    "    covtype_df_filtered.dropna(inplace=True)\n",
    "    covtype_df_filtered = covtype_df_filtered.sample(frac=1,random_state=10).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    num_anom = int(num_samples_covtype*(anom_percentage_covtype/100))\n",
    "    num_clean = int(num_samples_covtype-num_anom)\n",
    "\n",
    "    anoms=covtype_df_filtered[covtype_df_filtered['target']==1].iloc[0:num_anom]\n",
    "    clean = covtype_df_filtered[covtype_df_filtered['target']==0].iloc[0:num_clean]\n",
    "    frames = [anoms,clean]\n",
    "    print('Stream contains {} anomalies and {} no-anomalies'.format(anoms['target'].count(),clean['target'].count()))\n",
    "\n",
    "    final_set = pd.concat(frames,ignore_index=True)\n",
    "\n",
    "\n",
    "    x_covtype=final_set.iloc[:,:-1].transpose().to_dict()\n",
    "    y_covtype=final_set.iloc[:,-1].transpose().to_dict()\n",
    "    final_set = final_set.sample(frac=1,random_state=10).reset_index(drop=True)\n",
    "\n",
    "    frames_test_test=pd.DataFrame(data=[x_covtype,y_covtype]).transpose()\n",
    "    final_set=frames_test_test.copy()\n",
    "\n",
    "    #final_set_x=final_set.iloc[:,:-1]\n",
    "    #final_set_y=final_set.iloc[:,-1:]\n",
    "\n",
    "    return final_set,'covertpye', anom_percentage_covtype, num_samples_covtype, num_anom_classes_covtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit Card Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create dataset out of CreditCard dataset, works with multiplaying anomalies, random shuffles of all anomalies, take according number of anomalies, fitting dataset into stream format\n",
    "data_stream = stream.shuffle(river.datasets.CreditCard(), N_SAMPLES, seed=SEED)\n",
    "data1 = pd.DataFrame(data=data_stream)\n",
    "df_test= pd.DataFrame.from_dict(data1)\n",
    "anoms=df_test[df_test[1]==1]\n",
    "for i in range(0,10):\n",
    "    df_test=df_test.append(anoms)\n",
    "    i=i+1\n",
    "df_test = df_test.sample(frac=1,random_state=10).reset_index(drop=True)\n",
    "\n",
    "def make_credit_dataset(anom_percentage_credit,num_samples_credit,df_test):\n",
    "    num_anom = int(num_samples_credit*(anom_percentage_credit/100))\n",
    "    num_clean = int(num_samples_credit-num_anom)\n",
    "    anoms=df_test[df_test[1]==1].iloc[0:num_anom]\n",
    "    clean = df_test[df_test[1]==0].iloc[0:num_clean]\n",
    "    frames = [anoms,clean]\n",
    "    print('Stream contains {} anomalies and {} no-anomalies'.format(anoms[0].count(),clean[0].count()))\n",
    "    final_set = pd.concat(frames,ignore_index=True)\n",
    "    final_set = final_set.sample(frac=1,random_state=10).reset_index(drop=True)    \n",
    "    return final_set,'creditcard', anom_percentage_credit, num_samples_credit, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undercomplete Autoencoder standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undercomplete_ae(n_features, latent_dim=LATENT_DIM):\n",
    "    net = nn.Sequential(\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(n_features, 10),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(10, latent_dim),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(latent_dim, 10),\n",
    "        nn.LeakyReLU(), \n",
    "        nn.Linear(10, n_features),\n",
    "    )\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Half Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def stacked_ae(n_features, latent_dim=LATENT_DIM):\n",
    "    net = nn.Sequential(\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(n_features, 10),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(10, 5),\n",
    "        nn.LeakyReLU(),        \n",
    "        nn.Linear(5, 3),\n",
    "        nn.LeakyReLU(),        \n",
    "        nn.Linear(3, latent_dim),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(latent_dim, 3),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(3, 5),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(5, 10),\n",
    "        nn.LeakyReLU(),               \n",
    "        nn.Linear(10, n_features) \n",
    "    )\n",
    "    return net\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_ae(n_features, latent_dim=LATENT_DIM):\n",
    "    net = nn.Sequential(\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(n_features, 20),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(20, 10),\n",
    "        nn.LeakyReLU(),        \n",
    "        nn.Linear(10, 5),\n",
    "        nn.LeakyReLU(),        \n",
    "        nn.Linear(5, latent_dim),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(latent_dim, 5),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(5, 10),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(10, 20),\n",
    "        nn.LeakyReLU(),               \n",
    "        nn.Linear(20, n_features) \n",
    "    )\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-AE-(Incremental Baselines)\n",
    "## Tests before evaluation, results based on \"Evaluation\" Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneClassSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getestet mit folgenden Scalern und q-Werten, allerdings keine vernünftigen Ergebnisse bekommen\n",
    "'''#funktioniert nur mit river=0.9, allerdings muss dann git repo geupdatet werden, da anomalie.anomaliedetector klasse nicht mehr in base sondern in anomly ist --> Wheels können nicht mehr so gebaut werden wie bisher\n",
    "nu_given=[True,False]\n",
    "with_standardscaler=[True,False]\n",
    "anom_percentage_credit=1\n",
    "\n",
    "qq=[0.95,0.99,0.995]\n",
    "for q_value in qq:\n",
    "    for with_standardscaler_var in with_standardscaler:\n",
    "        for nu_given_var in nu_given:\n",
    "            if nu_given_var and with_standardscaler_var:\n",
    "                    model4 = compose.Pipeline(\n",
    "                    preprocessing.StandardScaler(),\n",
    "                    fx.RBFSampler(),\n",
    "                    anomaly.QuantileThresholder(\n",
    "                        anomaly.OneClassSVM(nu=anom_percentage_credit/100),\n",
    "                        q=q_value #q Anpassung viele Auswirkungen\n",
    "                    )\n",
    "                    )\n",
    "            if nu_given_var and not with_standardscaler_var:\n",
    "                    model4 = compose.Pipeline(\n",
    "        #            preprocessing.StandardScaler(),\n",
    "                    fx.RBFSampler(),\n",
    "                    anomaly.QuantileThresholder(\n",
    "                        anomaly.OneClassSVM(nu=anom_percentage_credit/100),\n",
    "                        q=q_value #q Anpassung viele Auswirkungen\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            if not nu_given_var and with_standardscaler_var:\n",
    "                model4 = compose.Pipeline(\n",
    "                    preprocessing.StandardScaler(),\n",
    "                    fx.RBFSampler(),    \n",
    "                    anomaly.QuantileThresholder(\n",
    "                        anomaly.OneClassSVM(),\n",
    "                        q=q_value #q Anpassung viele Auswirkungen\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "            if not nu_given_var and not with_standardscaler_var:\n",
    "                model4 = compose.Pipeline(\n",
    "    #                preprocessing.StandardScaler(),\n",
    "                    fx.RBFSampler(),    \n",
    "                    anomaly.QuantileThresholder(\n",
    "                        anomaly.OneClassSVM(),\n",
    "                        q=q_value #q Anpassung viele Auswirkungen\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            rocauc= river.metrics.ROCAUC()\n",
    "            j=0\n",
    "            data_stream = stream.shuffle(make_credit_dataset(1,5000,df_test)[0].itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "            for x, y in data_stream:\n",
    "                model4.learn_one(x)\n",
    "                y_pred= model4.score_one(x)\n",
    "                rocauc.update(y,y_pred)\n",
    "                if j<5:\n",
    "                    print(y_pred)\n",
    "                    print(y)\n",
    "                    j=j+1\n",
    "            print(rocauc)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HalfSpaceTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model3 = compose.Pipeline(\n",
    "    preprocessing.MinMaxScaler(),\n",
    "    anomaly.HalfSpaceTrees(seed=SEED)\n",
    ")\n",
    "## gibt Anomalie Score aus\n",
    "\n",
    "rocauc= river.metrics.ROCAUC()\n",
    "#data_stream = stream.shuffle(river.datasets.CreditCard().take(8000), N_SAMPLES, seed=42)\n",
    "j=0\n",
    "#data_stream = stream.shuffle(make_credit_dataset(10,10000,df_test)[0].itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "data_stream = stream.shuffle(dataset[0].itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "counter_supervised=0\n",
    "ls=False\n",
    "model3 = compose.Pipeline(\n",
    "    preprocessing.MinMaxScaler(),\n",
    "    anomaly.HalfSpaceTrees(seed=SEED)\n",
    ")\n",
    "for x, y in data_stream:\n",
    "    #model3.learn_one(x)\n",
    "    #y_pred= model3.score_one(x)\n",
    "#    if ls and y==0:\n",
    "#        model3.learn_one(x)\n",
    "    if not ls or counter_supervised==0:\n",
    "        model3.learn_one(x)\n",
    "        counter_supervised=counter_supervised+1\n",
    "    y_pred= model3.score_one(x) #high score means outlier\n",
    "    if j<5:\n",
    "        print(y_pred)\n",
    "        print(y)\n",
    "        j=j+1\n",
    "    rocauc.update(y,y_pred)\n",
    "#evaluation.append(rocauc)\n",
    "print(rocauc)\n",
    "evaluation=evaluation.append({\n",
    "    'model':'HalfSpaceTrees_min_max_scaler',\n",
    "    'anom_percentage_credit':anom_percentage_credit,\n",
    "    'num_samples_credit':num_samples_credit,\n",
    "    'Loss':LOSS,\n",
    "    'Optimizer':OPTIMIZER,\n",
    "    'Batch_size':BATCH_SIZE,\n",
    "    'Learning_rate':LEARNING_RATE,\n",
    "    'Latent_dim': LATENT_DIM,\n",
    "    'ROC':rocauc,\n",
    "    #'Precision_Recall_Curve':45,\n",
    "    #'FP':5,\n",
    "    #'TP': 4,\n",
    "    #'FN':8,\n",
    "    #'TN':10\n",
    "\n",
    "},ignore_index=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDRegressor (SVM-like with Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data_stream = stream.shuffle(make_credit_dataset(10,5000,df_test)[0].itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "\n",
    "\n",
    "#SGDRegressor (SVM-like with Kernel)\n",
    "model6=compose.Pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    fx.RBFSampler(),\n",
    "    compat.convert_sklearn_to_river(SGDClassifier(),classes=[False, True]),\n",
    "    )\n",
    "\n",
    "#HoeffdingAdaptiveTreeClassifier\n",
    "model8 = compose.Pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    HoeffdingAdaptiveTreeClassifier()\n",
    ")\n",
    "\n",
    "#Logistic Regression\n",
    "model9 = compose.Pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    linear_model.LogisticRegression(optimizer=op.SGD(0.1)))\n",
    "\n",
    "rocauc= river.metrics.ROCAUC(n_thresholds=100)\n",
    "\n",
    "for x, y in data_stream:\n",
    "    #y_pred= model3.predict_one(x)\n",
    "    model3.learn_one(x,y)       \n",
    "    rocauc.update(y,y_pred)\n",
    "\n",
    "rocauc\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HoeffdingAdaptiveTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''model8 = compose.Pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    HoeffdingAdaptiveTreeClassifier()\n",
    ")\n",
    "\n",
    "rocauc= river.metrics.ROCAUC()\n",
    "#data_stream = stream.shuffle(river.datasets.CreditCard().take(8000), N_SAMPLES, seed=42)\n",
    "i=0\n",
    "j=0\n",
    "data_stream = stream.shuffle(final_set.itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "for x, y in data_stream:\n",
    "    #print(y)\n",
    "    if i==0:\n",
    "        model8.learn_one(x,y)\n",
    "        i=i+1\n",
    "    y_pred=model8.predict_one(x)\n",
    "    #print(y_pred)\n",
    "    model8.learn_one(x,y)\n",
    "    rocauc.update(y,y_pred)\n",
    "    if j<5:\n",
    "        print(y_pred)\n",
    "        print(y)\n",
    "        j=j+1\n",
    "rocauc\n",
    "evaluation=evaluation.append({\n",
    "    'model':'HalfSpaceTrees_min_max_scaler',\n",
    "    'anom_percentage_credit':anom_percentage_credit,\n",
    "    'num_samples_credit':num_samples_credit,\n",
    "    'Loss':LOSS,\n",
    "    'Optimizer':OPTIMIZER,\n",
    "    'Batch_size':BATCH_SIZE,\n",
    "    'Learning_rate':LEARNING_RATE,\n",
    "    'Latent_dim': LATENT_DIM,\n",
    "    'ROC':rocauc,\n",
    "    #'Precision_Recall_Curve':45,\n",
    "    #'FP':5,\n",
    "    #'TP': 4,\n",
    "    #'FN':8,\n",
    "    #'TN':10\n",
    "\n",
    "},ignore_index=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model9 = compose.Pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    linear_model.LogisticRegression(optimizer=op.SGD(0.1))\n",
    ")\n",
    "\n",
    "rocauc= river.metrics.ROCAUC()\n",
    "i=0\n",
    "#data_stream = stream.shuffle(river.datasets.CreditCard().take(8000), N_SAMPLES, seed=42)\n",
    "data_stream = stream.shuffle(final_set.itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "for x, y in data_stream:\n",
    "    y_pred=model9.predict_one(x)\n",
    "    model9.learn_one(x,y)\n",
    "    rocauc.update(y,y_pred)\n",
    "    if i<5:\n",
    "        print(y_pred)\n",
    "        print(y)\n",
    "        i=i+1\n",
    "#evaluation.append(rocauc)\n",
    "rocauc\n",
    "evaluation=evaluation.append({\n",
    "    'model':'HalfSpaceTrees_min_max_scaler',\n",
    "    'anom_percentage_credit':anom_percentage_credit,\n",
    "    'num_samples_credit':num_samples_credit,\n",
    "    'Loss':LOSS,\n",
    "    'Optimizer':OPTIMIZER,\n",
    "    'Batch_size':BATCH_SIZE,\n",
    "    'Learning_rate':LEARNING_RATE,\n",
    "    'Latent_dim': LATENT_DIM,\n",
    "    'ROC':rocauc,\n",
    "    #'Precision_Recall_Curve':45,\n",
    "    #'FP':5,\n",
    "    #'TP': 4,\n",
    "    #'FN':8,\n",
    "    #'TN':10\n",
    "\n",
    "},ignore_index=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covtype Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "counter_supervised=0\n",
    "#for i in range(0,anom_percentage_credit.__len__()):\n",
    "#    datasets_credit.append(make_credit_dataset(anom_percentage_credit[i],num_samples_credit,df_test))\n",
    "learn_supervised=[True,False]\n",
    "\n",
    "for i in range(0,anom_percentage_covtype.__len__()):\n",
    "    for j in range(0,num_anom_classes_covtype.__len__()):\n",
    "        dataset=make_covtype_dataset(anom_percentage_covtype[i],num_samples_covtype,num_anom_classes_covtype[j])\n",
    "        ls=False\n",
    "        for ls in learn_supervised:\n",
    "            stacked_ae_model = compose.Pipeline(\n",
    "            preprocessing.StandardScaler(),\n",
    "            TorchAE(\n",
    "                        build_fn = stacked_ae,\n",
    "                        loss_fn = LOSS,\n",
    "                        optimizer_fn = OPTIMIZER,\n",
    "                        learning_rate=LEARNING_RATE,\n",
    "                        seed=SEED\n",
    "                )\n",
    "            )\n",
    "            undercomplete_ae_model = compose.Pipeline(\n",
    "            preprocessing.StandardScaler(),\n",
    "            TorchAE(\n",
    "                        build_fn = undercomplete_ae,\n",
    "                        loss_fn = LOSS,\n",
    "                        optimizer_fn = OPTIMIZER,\n",
    "                        learning_rate=LEARNING_RATE,\n",
    "                        seed=SEED\n",
    "                    )\n",
    "                )\n",
    "            #HalfSpaceTrees\n",
    "\n",
    "            halfSpaceTrees = compose.Pipeline(\n",
    "                preprocessing.MinMaxScaler(),\n",
    "                anomaly.HalfSpaceTrees(seed=SEED)\n",
    "            )\n",
    "            #SGDRegressor (SVM-like with Kernel)\n",
    "            sgdregressor=compose.Pipeline(\n",
    "                preprocessing.StandardScaler(),\n",
    "                fx.RBFSampler(),\n",
    "                compat.convert_sklearn_to_river(SGDClassifier(),classes=[False, True]),\n",
    "                )\n",
    "            #HoeffdingAdaptiveTreeClassifier\n",
    "            hoeffdingAdaptiveTreeClassifier = compose.Pipeline(\n",
    "                preprocessing.StandardScaler(),\n",
    "                HoeffdingAdaptiveTreeClassifier()\n",
    "            )\n",
    "            #Logistic Regression\n",
    "            logistic_Regression = compose.Pipeline(\n",
    "                preprocessing.StandardScaler(),\n",
    "                linear_model.LogisticRegression(optimizer=op.SGD(0.1)))\n",
    "            #models = [undercomplete_ae_model,stacked_ae_model,halfSpaceTrees,sgdregressor,hoeffdingAdaptiveTreeClassifier,logistic_Regression]\n",
    "            #models = [undercomplete_ae_model,stacked_ae_model]\n",
    "            models = [halfSpaceTrees]\n",
    "            model_counter=0    \n",
    "            for model in models:          \n",
    "                counter_supervised=0\n",
    "                print(model_counter)\n",
    "                if model_counter==0:\n",
    "                    #model_name='undercomplete_ae_model'\n",
    "                    model_name='halfSpaceTrees_no2'\n",
    "                if model_counter==1:\n",
    "                    model_name='stacked_ae_model'\n",
    "                if model_counter==2:\n",
    "                    model_name='halfSpaceTrees'\n",
    "                if model_counter==3:\n",
    "                    model_name='sgdregressor'\n",
    "                if model_counter==4:\n",
    "                    model_name='hoeffdingAdaptiveTreeClassifier'\n",
    "                if model_counter==5:\n",
    "                    model_name='logistic_Regression'\n",
    "\n",
    "                rocauc= river.metrics.ROCAUC(n_thresholds=100)\n",
    "                data_stream = stream.shuffle(dataset[0].itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "                learning_counter=0\n",
    "                if model_counter<3:\n",
    "                    for x, y in data_stream:\n",
    "                        if learning_counter==0:\n",
    "                            model.learn_one(x)\n",
    "                            learning_counter=learning_counter+1\n",
    "                        y_pred= model.score_one(x) #high score means outlier\n",
    "                        if ls and y==0:\n",
    "                            model.learn_one(x)\n",
    "                        if not ls or counter_supervised==0:\n",
    "                            model.learn_one(x)\n",
    "                            counter_supervised=counter_supervised+1\n",
    "                        y_pred= model.score_one(x) #high score means outlier\n",
    "                        #if j<3:\n",
    "                        #    print(y_pred)\n",
    "                        #    print(y)\n",
    "                        #    j=j+1\n",
    "                        rocauc.update(y,y_pred)\n",
    "                if model_counter>2 and ls:\n",
    "                    for x, y in data_stream:\n",
    "                        if learning_counter==0:\n",
    "                            model.learn_one(x,y)\n",
    "                            learning_counter=learning_counter+1\n",
    "                        y_pred=model.predict_one(x)\n",
    "                        model.learn_one(x,y)\n",
    "                        rocauc.update(y,y_pred)\n",
    "\n",
    "                model_counter=model_counter+1                 \n",
    "                print(model_name)\n",
    "                print(rocauc)\n",
    "                evaluation=evaluation.append({\n",
    "                    'model': model_name,\n",
    "                    'num_neurons': [10,5,3],\n",
    "                    'dataset': dataset[1],\n",
    "                    'anom_percentage':dataset[2],\n",
    "                    'num_samples':dataset[3],\n",
    "                    'num_anom_classes':dataset[4],\n",
    "                    'learn_supervised':ls,\n",
    "                    'Loss':LOSS,\n",
    "                    'Optimizer':OPTIMIZER,\n",
    "                    'Batch_size':BATCH_SIZE,\n",
    "                    'Learning_rate':LEARNING_RATE,\n",
    "                    'Latent_dim': LATENT_DIM,\n",
    "                    'ROC':rocauc,\n",
    "                },ignore_index=True)\n",
    "\n",
    "#evaluation.to_csv('evaluation_8_halfSpaceTrees.vers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creditcard Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "counter_supervised=0\n",
    "#for i in range(0,anom_percentage_credit.__len__()):\n",
    "#    datasets_credit.append(make_credit_dataset(anom_percentage_credit[i],num_samples_credit,df_test))\n",
    "learn_supervised=[True,False]\n",
    "\n",
    "for i in range(0,anom_percentage_credit.__len__()):\n",
    "    for j in range(0,num_anom_classes_covtype.__len__()):\n",
    "        dataset=make_credit_dataset(anom_percentage_credit[i],num_samples_credit,df_test)\n",
    "        for ls in learn_supervised:\n",
    "            stacked_ae_model = compose.Pipeline(\n",
    "            preprocessing.StandardScaler(),\n",
    "            TorchAE(\n",
    "                        build_fn = stacked_ae,\n",
    "                        loss_fn = LOSS,\n",
    "                        optimizer_fn = OPTIMIZER,\n",
    "                        learning_rate=LEARNING_RATE,\n",
    "                        seed=SEED\n",
    "                )\n",
    "            )\n",
    "            undercomplete_ae_model = compose.Pipeline(\n",
    "            preprocessing.StandardScaler(),\n",
    "            TorchAE(\n",
    "                        build_fn = undercomplete_ae,\n",
    "                        loss_fn = LOSS,\n",
    "                        optimizer_fn = OPTIMIZER,\n",
    "                        learning_rate=LEARNING_RATE,\n",
    "                        seed=SEED\n",
    "                    )\n",
    "                )\n",
    "            halfSpaceTrees = compose.Pipeline(\n",
    "                preprocessing.MinMaxScaler(),\n",
    "                anomaly.HalfSpaceTrees(seed=SEED)\n",
    "            )\n",
    "            #SGDRegressor (SVM-like with Kernel)\n",
    "            sgdregressor=compose.Pipeline(\n",
    "                preprocessing.StandardScaler(),\n",
    "                fx.RBFSampler(),\n",
    "                compat.convert_sklearn_to_river(SGDClassifier(),classes=[False, True]),\n",
    "                )\n",
    "            #HoeffdingAdaptiveTreeClassifier\n",
    "            hoeffdingAdaptiveTreeClassifier = compose.Pipeline(\n",
    "                preprocessing.StandardScaler(),\n",
    "                HoeffdingAdaptiveTreeClassifier()\n",
    "            )\n",
    "            #Logistic Regression\n",
    "            logistic_Regression = compose.Pipeline(\n",
    "                preprocessing.StandardScaler(),\n",
    "                linear_model.LogisticRegression(optimizer=op.SGD(0.1)))\n",
    "            #models = [undercomplete_ae_model,stacked_ae_model,halfSpaceTrees,sgdregressor,hoeffdingAdaptiveTreeClassifier,logistic_Regression]\n",
    "            #models = [undercomplete_ae_model,stacked_ae_model]\n",
    "            models= [halfSpaceTrees]\n",
    "            model_counter=0    \n",
    "            for model in models:          \n",
    "                counter_supervised=0\n",
    "                print(model_counter)\n",
    "                if model_counter==0:\n",
    "                    model_name='undercomplete_ae_model'\n",
    "                if model_counter==1:\n",
    "                    model_name='stacked_ae_model'\n",
    "                if model_counter==2:\n",
    "                    model_name='halfSpaceTrees'\n",
    "                if model_counter==3:\n",
    "                    model_name='sgdregressor'\n",
    "                if model_counter==4:\n",
    "                    model_name='hoeffdingAdaptiveTreeClassifier'\n",
    "                if model_counter==5:\n",
    "                    model_name='logistic_Regression'\n",
    "\n",
    "                rocauc= river.metrics.ROCAUC(n_thresholds=100)\n",
    "                data_stream = stream.shuffle(dataset[0].itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "                learning_counter=0\n",
    "                counter1=0\n",
    "                counter2 =0                \n",
    "                if model_counter<3:\n",
    "                    for x, y in data_stream:\n",
    "                        if learning_counter==0:\n",
    "                            model.learn_one(x)\n",
    "                            learning_counter=learning_counter+1\n",
    "                        y_pred= model.score_one(x) #high score means outlier\n",
    "                        if ls and y==0:\n",
    "                            model.learn_one(x)\n",
    "                        if not ls or counter_supervised==0:\n",
    "                            model.learn_one(x)\n",
    "                            counter_supervised=counter_supervised+1\n",
    "                        y_pred= model.score_one(x) #high score means outlier\n",
    "\n",
    "                        if y==1:\n",
    "                            counter1=counter1+1\n",
    "                        if y==0:\n",
    "                            counter2=counter2+1\n",
    "                        #if j<3:\n",
    "                        #    print(y_pred)\n",
    "                        #    print(y)\n",
    "                        #    j=j+1\n",
    "                        rocauc.update(y,y_pred)\n",
    "                    print(counter1)\n",
    "                    print(counter2)\n",
    "\n",
    "                if model_counter>2 and ls:\n",
    "                    for x, y in data_stream:\n",
    "                        if learning_counter==0:\n",
    "                            model.learn_one(x,y)\n",
    "                            learning_counter=learning_counter+1\n",
    "                        y_pred=model.predict_one(x)\n",
    "                        model.learn_one(x,y)\n",
    "                        rocauc.update(y,y_pred)\n",
    "\n",
    "                model_counter=model_counter+1                 \n",
    "                print(model_name)\n",
    "                print(rocauc)\n",
    "                evaluation=evaluation.append({\n",
    "                    'model': model_name,\n",
    "                    'num_neurons': [10,5,3],\n",
    "                    'dataset': dataset[1],\n",
    "                    'anom_percentage':dataset[2],\n",
    "                    'num_samples':dataset[3],\n",
    "                    'num_anom_classes':dataset[4],\n",
    "                    'learn_supervised':ls,\n",
    "                    'Loss':LOSS,\n",
    "                    'Optimizer':OPTIMIZER,\n",
    "                    'Batch_size':BATCH_SIZE,\n",
    "                    'Learning_rate':LEARNING_RATE,\n",
    "                    'Latent_dim': LATENT_DIM,\n",
    "                    'ROC':rocauc,\n",
    "                },ignore_index=True)\n",
    "#evaluation.to_csv('evaluation_9_halfSpaceTrees.vers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random/Backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ipca = IncrementalPCA(n_components=LATENT_DIM)\n",
    "model6=compose.Pipeline(\n",
    "    preprocessing.MinMaxScaler(),\n",
    "    ipca #IncrementalPCA(n_components=LATENT_DIM, batch_size=1)\n",
    ")\n",
    "#model7=compose.Pipeline(\n",
    "#    preprocessing.MinMaxScaler(),\n",
    "#    SklearnAnomalyDetector(IncrementalPCA(n_components=10))\n",
    "#)\n",
    "#rocauc= river.metrics.ROCAUC()\n",
    "#data_stream = stream.shuffle(river.datasets.CreditCard().take(8000), N_SAMPLES, seed=SEED)\n",
    "#for x, y in data_stream:\n",
    "#    print(x.values())\n",
    "#data_stream = stream.shuffle(river.datasets.CreditCard().take(20000), N_SAMPLES, seed=SEED)\n",
    "data_stream = stream.shuffle(final_set.itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "#data_stream = list(data_stream)\n",
    "\n",
    "data1 = pd.DataFrame(data=data_stream)\n",
    "#data2 = pd.DataFrame.from_dict(data=data1,orient='index')\n",
    "#test=data1[0].to_dict()\n",
    "#df_test=pd.DataFrame.from_dict(test, orient='index')\n",
    "#df_test\n",
    "#pd.DataFrame.from_dict({(i,j): data1[i][j] \n",
    "#                           for i in data1.keys() \n",
    "#                           for j in data1[i].keys()},\n",
    "#                       orient='index')\n",
    "#data_stream = stream.shuffle(river.datasets.CreditCard().take(20000), N_SAMPLES, seed=SEED)\n",
    "data_stream = stream.shuffle(final_set.itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "data1 = pd.DataFrame(data=data_stream)\n",
    "df_test= pd.DataFrame.from_dict(data1)\n",
    "\n",
    "\n",
    "data_stream1 = stream.shuffle(df_test.itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "#data_stream = stream.shuffle(river.datasets.CreditCard().take(8000), N_SAMPLES, seed=SEED)\n",
    "data_stream = stream.shuffle(final_set.itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "#df=pd.read_csv(r'C:\\Users\\Manuel\\river_data\\CreditCard\\creditcard.csv')\n",
    "rocauc= river.metrics.ROCAUC()\n",
    "#data_stream = stream.shuffle(river.datasets.CreditCard().take(5000), N_SAMPLES, seed=40)\n",
    "MSE_score_arr = []\n",
    "values = []\n",
    "for x, y in data_stream:\n",
    "    #print(pd.DataFrame.from_dict(x))\n",
    "    ipca = ipca.partial_fit(pd.DataFrame.from_dict([x]))\n",
    "    x_trans = ipca.transform(pd.DataFrame.from_dict([x]))\n",
    "    inverse_trans= ipca.inverse_transform(x_trans)\n",
    "    MSE_score = ((np.array(pd.DataFrame.from_dict([x]))-np.array([inverse_trans]))**2).sum()\n",
    "    MSE_score_arr.append(MSE_score)\n",
    "    values.append(y)\n",
    "#    model7.learn_one(x)\n",
    "#    y_pred= model7.score_one(x)\n",
    "#    rocauc.update(y,y_pred)\n",
    "rocauc\n",
    "values_df = pd.Series(data=values)\n",
    "loss = pd.Series(data=MSE_score_arr)\n",
    "loss = (loss-np.min(loss))/(np.max(loss)-np.min(loss))\n",
    "loss.sum()\n",
    "rocauc = metrics.ROCAUC()\n",
    "for yt, yp in zip(values, loss):\n",
    "    #print(yt, yp)\n",
    "    rocauc = rocauc.update(yt, yp)\n",
    "rocauc\n",
    "#evaluation.append(rocauc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dataset = datasets.synth.RandomRBF(seed_model=7, seed_sample=SEED,n_classes=2,n_features=200).take(N_SAMPLES)\n",
    "data_stream = stream.shuffle(dataset,buffer_size=N_SAMPLES)\n",
    "\n",
    "y_count=0\n",
    "y_0_count=0\n",
    "\n",
    "def build_fn(n_features):\n",
    "    net = nn.Sequential(\n",
    "        nn.Linear(n_features, 5),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(5, 5),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(5, 5),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(5, 5),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(5, 1),\n",
    "        nn.Softmax()\n",
    "    )\n",
    "    return net\n",
    "\n",
    "model = compose.Pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    PyTorch2RiverClassifier(\n",
    "                build_fn=build_fn,\n",
    "                loss_fn=nn.BCELoss,\n",
    "                optimizer_fn=optim.Adam,\n",
    "                batch_size=1,\n",
    "                learning_rate=1e-3,\n",
    "    )\n",
    ")\n",
    "\n",
    "for x, y in data_stream:\n",
    "    y_pred = model.predict_proba_one(x)\n",
    "    model.learn_one(x)\n",
    "    rocauc.update(y, y_pred)\n",
    "    #print(y)\n",
    "    if y==1:\n",
    "        y_count+=1\n",
    "    else:\n",
    "        y_0_count+=1\n",
    "rocauc'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtergezogenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''undercomplete_ae_model = compose.Pipeline(\n",
    "    #preprocessing.StandardScaler(),\n",
    "    compat.PyTorch2RiverClassifier(\n",
    "                build_fn = undercomplete_ae,\n",
    "                loss_fn = nn.BCELoss,\n",
    "                optimizer_fn = OPTIMIZER,\n",
    "                #batch_size=BATCH_SIZE,\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                seed=SEED\n",
    "    )\n",
    ")\n",
    "'''\n",
    "'''undercomplete_ae_model = Autoencoder(\n",
    "                build_fn = undercomplete_ae,\n",
    "                loss_fn = LOSS,\n",
    "                optimizer_fn = OPTIMIZER,\n",
    "                #batch_size=BATCH_SIZE,\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                seed=SEED\n",
    "    )\n",
    "'''\n",
    "'''undercomplete_ae_model = compose.Pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    PyTorch2RiverClassifier(\n",
    "                build_fn = undercomplete_ae,\n",
    "                loss_fn = nn.MSELoss,\n",
    "                optimizer_fn = OPTIMIZER,\n",
    "                #batch_size=BATCH_SIZE,\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                seed=SEED\n",
    "    )\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD Undercomplete Autoencoder with Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def undercomplete_ae_sm(n_features, latent_dim=LATENT_DIM):\n",
    "    net = nn.Sequential(\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(n_features, 20), \n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(20, latent_dim),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(latent_dim, 20),\n",
    "        nn.LeakyReLU(), \n",
    "        nn.Linear(20, n_features),\n",
    "        nn.Linear(n_features,1),\n",
    "        nn.Softmax()\n",
    "    )\n",
    "    return net\n",
    "    '''\n",
    "\n",
    "'''undercomplete_ae_sm_model = compose.Pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    PyTorch2RiverClassifier(\n",
    "                build_fn = undercomplete_ae_sm,\n",
    "                loss_fn = LOSS,\n",
    "                optimizer_fn = OPTIMIZER,\n",
    "                #batch_size=BATCH_SIZE,\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                seed=SEED\n",
    "    )\n",
    ")\n",
    "'''\n",
    "\n",
    "'''#supervised learning approach with Softmax function --> Proba gets predicted, ROC way worse\n",
    "rocauc = river.metrics.ROCAUC()\n",
    "#data_stream = stream.shuffle(river.datasets.CreditCard().take(8000), N_SAMPLES, seed=42)\n",
    "data_stream = stream.shuffle(final_set.itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "for x, y in data_stream:\n",
    "    y_pred = undercomplete_ae_sm_model.predict_proba_one(x) #ruft learn_unsupervised auf, müssen wir learn_one dann überhaupt auch aufrufen?\n",
    "    undercomplete_ae_sm_model.learn_one(x, y) #undercomplete_ae_sm_model.learn_one ist supervised Ansatz, wir wollen Unsupervised\n",
    "    rocauc.update(y, y_pred)\n",
    "rocauc\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''## backup if anything fails\n",
    "rocauc= river.metrics.ROCAUC(n_thresholds=100)\n",
    "j=0\n",
    "learn_supervised=[True,False]\n",
    "models = [undercomplete_ae_model,stacked_ae_model]\n",
    "#learn_supervised=True\n",
    "y_pred_arr_undercomplete_ae = []\n",
    "y_act_arr_undercomplete_ae = []\n",
    "y_pred_cf=[]\n",
    "#f1 = river.metrics.F1()\n",
    "#cm=river.metrics.ConfusionMatrix()\n",
    "#data_stream = stream.shuffle(river.datasets.CreditCard().take(8000), N_SAMPLES, seed=SEED)\n",
    "for model in models:    \n",
    "    for ls in learn_supervised:\n",
    "        data_stream = stream.shuffle(final_set.itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "        for x, y in data_stream:\n",
    "            if ls and y==0:\n",
    "                undercomplete_ae_model.learn_one(x,y)\n",
    "            else:\n",
    "                undercomplete_ae_model.learn_one(x)\n",
    "            y_pred= undercomplete_ae_model.score_one(x) #high score means outlier\n",
    "            if j<5:\n",
    "                print(y_pred)\n",
    "                print(y)\n",
    "                j=j+1\n",
    "            rocauc.update(y,y_pred)\n",
    "        #    if y_pred>threshhold:\n",
    "        #        y_pred=1\n",
    "        #    else:\n",
    "        #        y_pred=0\n",
    "            y_pred_arr_undercomplete_ae.append([y_pred,y])\n",
    "            y_pred_cf.append(y_pred)\n",
    "            y_act_arr_undercomplete_ae.append(y)\n",
    "\n",
    "            #f1.update(y,y_pred)\n",
    "            #cm.update(y,y_pred)\n",
    "            #undercomplete_ae_model.learn_one(x,y_pred)\n",
    "            #undercomplete_ae_model.learn_one(x,learn_unsupervised=True)\n",
    "            #y_pred = undercomplete_ae_sm_model.predict_proba_one(x) #ruft learn_unsupervised auf, müssen wir learn_one dann überhaupt auch aufrufen?\n",
    "            #undercomplete_ae_model.learn_one(x, y)\n",
    "            #rocauc.update(y, y_pred)\n",
    "        print(rocauc)\n",
    "        evaluation=evaluation.append({\n",
    "            'model':'undercomplete_ae_model',\n",
    "            'anom_percentage_credit':anom_percentage_credit,\n",
    "            'num_samples_credit':num_samples_credit,\n",
    "            'Loss':LOSS,\n",
    "            'Optimizer':OPTIMIZER,\n",
    "            'Batch_size':BATCH_SIZE,\n",
    "            'Learning_rate':LEARNING_RATE,\n",
    "            'Latent_dim': LATENT_DIM,\n",
    "            'ROC':rocauc,\n",
    "            #'Precision_Recall_Curve':45,\n",
    "            #'FP':5,\n",
    "            #'TP': 4,\n",
    "            #'FN':8,\n",
    "            #'TN':10\n",
    "\n",
    "        },ignore_index=True)\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "43d093f43044a3a65e6980dc28089ab95a7d3d66b3284b0379db52f7d7b16821"
  },
  "kernelspec": {
   "display_name": "rwch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
