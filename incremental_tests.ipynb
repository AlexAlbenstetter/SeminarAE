{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "from torch import optim\n",
    "from river import compose, metrics, preprocessing, stream, anomaly, linear_model, datasets, compose\n",
    "from river import feature_extraction as fx\n",
    "from river.tree import HoeffdingAdaptiveTreeClassifier\n",
    "from river import optim as op\n",
    "\n",
    "from IncrementalTorch.anomaly.anomaly import Autoencoder, BasicAutoencoder\n",
    "from river import compat\n",
    "\n",
    "from tqdm import tqdm\n",
    "import river  \n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "from OnlineTorch.classifier import PyTorch2RiverClassifier\n",
    "from OnlineTorch.anomaly import TorchAE\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import SGDOneClassSVM\n",
    "from sklearn.cluster import k_means\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from river import evaluate\n",
    "\n",
    "\n",
    "N_SAMPLES = 5000 #number of elements stored in memory\n",
    "SEED = 42 #Random Seed for shuffles, constant\n",
    "#track_name = \"RBF\"\n",
    "#LOSS = nn.BCELoss\n",
    "LOSS = nn.L1Loss #Lossfunction, constant\n",
    "#LOSS = nn.CrossEntropyLoss\n",
    "OPTIMIZER = optim.AdamW #Optimizer, constant\n",
    "BATCH_SIZE=10 #Tracked for PCA, not necessary anymore\n",
    "LEARNING_RATE=1e-3 #Constant\n",
    "\n",
    "rocauc = river.metrics.ROCAUC() #metric for evaluation\n",
    "\n",
    "threshhold=0 #Tracked for Confusion Matrix, not necessary anymore\n",
    "\n",
    "########################################################################################\n",
    "#Latent Dim 1 or 2; both evaluated; \n",
    "LATENT_DIM = 1\n",
    "#######################################################################################\n",
    "\n",
    "##max 5% with 10000 samples\n",
    "anom_percentage_credit = [50,40,30,20,10,5,2.5,1.25,0.625,0.313,0.172] #everything evaluated\n",
    "num_samples_credit = 9840 #50% * 9840 --> fits with num anomalies\n",
    "\n",
    "anom_percentage_covtype = [50,40,30,20,10,5,2.5,1.25,0.625,0.313,0.172] #everything evaluated\n",
    "num_samples_covtype = 25000\n",
    "num_anom_classes_covtype=[1,2,3,4,5] #if you want 1 anom class type 1, up to 5 anomalie classes\n",
    "\n",
    "#Structure Result csv\n",
    "evaluation=pd.DataFrame(columns=['model','num_neurons','dataset','anom_percentage','num_samples','num_anom_classes','learn_supervised','Loss','Optimizer','Batch_size','Learning_rate','Latent_dim','ROC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covtype Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class 5 contains 9493 instances\n",
    "class 2 contains 283301 instances\n",
    "class 1 contains 211840 instances\n",
    "class 7 contains 20510 instances\n",
    "class 3 contains 35754 instances\n",
    "class 6 contains 17367 instances\n",
    "class 4 contains 2747 instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create dataset out of covertype dataset, working with random shuffles of all anomalie classes, take according number of anomalies, fitting dataset into stream format\n",
    "def make_covtype_dataset(anom_percentage_covtype,num_samples_covtype,num_anom_classes_covtype):  \n",
    "    covtype_x, covtype_y=fetch_covtype(as_frame=True,return_X_y=True)\n",
    "    covtype_df=covtype_x\n",
    "    covtype_df['target']=covtype_y\n",
    "    covtype_df['target']=covtype_df['target']-1\n",
    "    covtype_df_classes=covtype_df.target.unique()\n",
    "    for i in covtype_df_classes:\n",
    "        print('class {0} contains {1} instances'.format(i,covtype_df[covtype_df.target==i].Elevation.count()))\n",
    "\n",
    "    for i in range (1,num_anom_classes_covtype+1):\n",
    "        covtype_df.target[covtype_df.target==i]=1\n",
    "        covtype_df.loc[covtype_df['target']==i, 'target']=1\n",
    "\n",
    "    covtype_df_filtered=covtype_df[covtype_df.target<=num_anom_classes_covtype]\n",
    "    covtype_df_filtered.dropna(inplace=True)\n",
    "    covtype_df_filtered = covtype_df_filtered.sample(frac=1,random_state=10).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    num_anom = int(num_samples_covtype*(anom_percentage_covtype/100))\n",
    "    num_clean = int(num_samples_covtype-num_anom)\n",
    "\n",
    "    anoms=covtype_df_filtered[covtype_df_filtered['target']==1].iloc[0:num_anom]\n",
    "    clean = covtype_df_filtered[covtype_df_filtered['target']==0].iloc[0:num_clean]\n",
    "    frames = [anoms,clean]\n",
    "    print('Stream contains {} anomalies and {} no-anomalies'.format(anoms['target'].count(),clean['target'].count()))\n",
    "\n",
    "    final_set = pd.concat(frames,ignore_index=True)\n",
    "\n",
    "\n",
    "    x_covtype=final_set.iloc[:,:-1].transpose().to_dict()\n",
    "    y_covtype=final_set.iloc[:,-1].transpose().to_dict()\n",
    "    final_set = final_set.sample(frac=1,random_state=10).reset_index(drop=True)\n",
    "\n",
    "    frames_test_test=pd.DataFrame(data=[x_covtype,y_covtype]).transpose()\n",
    "    final_set=frames_test_test.copy()\n",
    "\n",
    "    #final_set_x=final_set.iloc[:,:-1]\n",
    "    #final_set_y=final_set.iloc[:,-1:]\n",
    "\n",
    "    return final_set,'covertpye', anom_percentage_covtype, num_samples_covtype, num_anom_classes_covtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit Card Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create dataset out of CreditCard dataset, works with multiplaying anomalies, random shuffles of all anomalies, take according number of anomalies, fitting dataset into stream format\n",
    "data_stream = stream.shuffle(river.datasets.CreditCard(), N_SAMPLES, seed=SEED)\n",
    "data1 = pd.DataFrame(data=data_stream)\n",
    "df_test= pd.DataFrame.from_dict(data1)\n",
    "anoms=df_test[df_test[1]==1]\n",
    "for i in range(0,10):\n",
    "    df_test=df_test.append(anoms)\n",
    "    i=i+1\n",
    "df_test = df_test.sample(frac=1,random_state=10).reset_index(drop=True)\n",
    "\n",
    "def make_credit_dataset(anom_percentage_credit,num_samples_credit,df_test):\n",
    "    num_anom = int(num_samples_credit*(anom_percentage_credit/100))\n",
    "    num_clean = int(num_samples_credit-num_anom)\n",
    "    anoms=df_test[df_test[1]==1].iloc[0:num_anom]\n",
    "    clean = df_test[df_test[1]==0].iloc[0:num_clean]\n",
    "    frames = [anoms,clean]\n",
    "    print('Stream contains {} anomalies and {} no-anomalies'.format(anoms[0].count(),clean[0].count()))\n",
    "    final_set = pd.concat(frames,ignore_index=True)\n",
    "    final_set = final_set.sample(frac=1,random_state=10).reset_index(drop=True)    \n",
    "    return final_set,'creditcard', anom_percentage_credit, num_samples_credit, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undercomplete Autoencoder standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undercomplete_ae(n_features, latent_dim=LATENT_DIM):\n",
    "    net = nn.Sequential(\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(n_features, 10),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(10, latent_dim),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(latent_dim, 10),\n",
    "        nn.LeakyReLU(), \n",
    "        nn.Linear(10, n_features),\n",
    "    )\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Half Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def stacked_ae(n_features, latent_dim=LATENT_DIM):\\n    net = nn.Sequential(\\n        nn.Dropout(),\\n        nn.Linear(n_features, 10),\\n        nn.LeakyReLU(),\\n        nn.Linear(10, 5),\\n        nn.LeakyReLU(),        \\n        nn.Linear(5, 3),\\n        nn.LeakyReLU(),        \\n        nn.Linear(3, latent_dim),\\n        nn.LeakyReLU(),\\n        nn.Linear(latent_dim, 3),\\n        nn.LeakyReLU(),\\n        nn.Linear(3, 5),\\n        nn.LeakyReLU(),\\n        nn.Linear(5, 10),\\n        nn.LeakyReLU(),               \\n        nn.Linear(10, n_features) \\n    )\\n    return net\\n    '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def stacked_ae(n_features, latent_dim=LATENT_DIM):\n",
    "    net = nn.Sequential(\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(n_features, 10),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(10, 5),\n",
    "        nn.LeakyReLU(),        \n",
    "        nn.Linear(5, 3),\n",
    "        nn.LeakyReLU(),        \n",
    "        nn.Linear(3, latent_dim),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(latent_dim, 3),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(3, 5),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(5, 10),\n",
    "        nn.LeakyReLU(),               \n",
    "        nn.Linear(10, n_features) \n",
    "    )\n",
    "    return net\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_ae(n_features, latent_dim=LATENT_DIM):\n",
    "    net = nn.Sequential(\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(n_features, 20),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(20, 10),\n",
    "        nn.LeakyReLU(),        \n",
    "        nn.Linear(10, 5),\n",
    "        nn.LeakyReLU(),        \n",
    "        nn.Linear(5, latent_dim),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(latent_dim, 5),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(5, 10),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(10, 20),\n",
    "        nn.LeakyReLU(),               \n",
    "        nn.Linear(20, n_features) \n",
    "    )\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-AE-(Incremental Baselines)\n",
    "## Tests before evaluation, results based on \"Evaluation\" Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneClassSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#funktioniert nur mit river=0.9, allerdings muss dann git repo geupdatet werden, da anomalie.anomaliedetector klasse nicht mehr in base sondern in anomly ist --> Wheels können nicht mehr so gebaut werden wie bisher\\nnu_given=[True,False]\\nwith_standardscaler=[True,False]\\nanom_percentage_credit=1\\n\\nqq=[0.95,0.99,0.995]\\nfor q_value in qq:\\n    for with_standardscaler_var in with_standardscaler:\\n        for nu_given_var in nu_given:\\n            if nu_given_var and with_standardscaler_var:\\n                    model4 = compose.Pipeline(\\n                    preprocessing.StandardScaler(),\\n                    fx.RBFSampler(),\\n                    anomaly.QuantileThresholder(\\n                        anomaly.OneClassSVM(nu=anom_percentage_credit/100),\\n                        q=q_value #q Anpassung viele Auswirkungen\\n                    )\\n                    )\\n            if nu_given_var and not with_standardscaler_var:\\n                    model4 = compose.Pipeline(\\n        #            preprocessing.StandardScaler(),\\n                    fx.RBFSampler(),\\n                    anomaly.QuantileThresholder(\\n                        anomaly.OneClassSVM(nu=anom_percentage_credit/100),\\n                        q=q_value #q Anpassung viele Auswirkungen\\n                    )\\n                )\\n\\n            if not nu_given_var and with_standardscaler_var:\\n                model4 = compose.Pipeline(\\n                    preprocessing.StandardScaler(),\\n                    fx.RBFSampler(),    \\n                    anomaly.QuantileThresholder(\\n                        anomaly.OneClassSVM(),\\n                        q=q_value #q Anpassung viele Auswirkungen\\n                    )\\n                )\\n            \\n            if not nu_given_var and not with_standardscaler_var:\\n                model4 = compose.Pipeline(\\n    #                preprocessing.StandardScaler(),\\n                    fx.RBFSampler(),    \\n                    anomaly.QuantileThresholder(\\n                        anomaly.OneClassSVM(),\\n                        q=q_value #q Anpassung viele Auswirkungen\\n                    )\\n                )\\n\\n            rocauc= river.metrics.ROCAUC()\\n            j=0\\n            data_stream = stream.shuffle(make_credit_dataset(1,5000,df_test)[0].itertuples(index=False),N_SAMPLES, seed=SEED)\\n            for x, y in data_stream:\\n                model4.learn_one(x)\\n                y_pred= model4.score_one(x)\\n                rocauc.update(y,y_pred)\\n                if j<5:\\n                    print(y_pred)\\n                    print(y)\\n                    j=j+1\\n            print(rocauc)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getestet mit folgenden Scalern und q-Werten, allerdings keine vernünftigen Ergebnisse bekommen\n",
    "'''#funktioniert nur mit river=0.9, allerdings muss dann git repo geupdatet werden, da anomalie.anomaliedetector klasse nicht mehr in base sondern in anomly ist --> Wheels können nicht mehr so gebaut werden wie bisher\n",
    "nu_given=[True,False]\n",
    "with_standardscaler=[True,False]\n",
    "anom_percentage_credit=1\n",
    "\n",
    "qq=[0.95,0.99,0.995]\n",
    "for q_value in qq:\n",
    "    for with_standardscaler_var in with_standardscaler:\n",
    "        for nu_given_var in nu_given:\n",
    "            if nu_given_var and with_standardscaler_var:\n",
    "                    model4 = compose.Pipeline(\n",
    "                    preprocessing.StandardScaler(),\n",
    "                    fx.RBFSampler(),\n",
    "                    anomaly.QuantileThresholder(\n",
    "                        anomaly.OneClassSVM(nu=anom_percentage_credit/100),\n",
    "                        q=q_value #q Anpassung viele Auswirkungen\n",
    "                    )\n",
    "                    )\n",
    "            if nu_given_var and not with_standardscaler_var:\n",
    "                    model4 = compose.Pipeline(\n",
    "        #            preprocessing.StandardScaler(),\n",
    "                    fx.RBFSampler(),\n",
    "                    anomaly.QuantileThresholder(\n",
    "                        anomaly.OneClassSVM(nu=anom_percentage_credit/100),\n",
    "                        q=q_value #q Anpassung viele Auswirkungen\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            if not nu_given_var and with_standardscaler_var:\n",
    "                model4 = compose.Pipeline(\n",
    "                    preprocessing.StandardScaler(),\n",
    "                    fx.RBFSampler(),    \n",
    "                    anomaly.QuantileThresholder(\n",
    "                        anomaly.OneClassSVM(),\n",
    "                        q=q_value #q Anpassung viele Auswirkungen\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "            if not nu_given_var and not with_standardscaler_var:\n",
    "                model4 = compose.Pipeline(\n",
    "    #                preprocessing.StandardScaler(),\n",
    "                    fx.RBFSampler(),    \n",
    "                    anomaly.QuantileThresholder(\n",
    "                        anomaly.OneClassSVM(),\n",
    "                        q=q_value #q Anpassung viele Auswirkungen\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            rocauc= river.metrics.ROCAUC()\n",
    "            j=0\n",
    "            data_stream = stream.shuffle(make_credit_dataset(1,5000,df_test)[0].itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "            for x, y in data_stream:\n",
    "                model4.learn_one(x)\n",
    "                y_pred= model4.score_one(x)\n",
    "                rocauc.update(y,y_pred)\n",
    "                if j<5:\n",
    "                    print(y_pred)\n",
    "                    print(y)\n",
    "                    j=j+1\n",
    "            print(rocauc)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HalfSpaceTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel3 = compose.Pipeline(\\n    preprocessing.MinMaxScaler(),\\n    anomaly.HalfSpaceTrees(seed=SEED)\\n)\\n## gibt Anomalie Score aus\\n\\nrocauc= river.metrics.ROCAUC()\\n#data_stream = stream.shuffle(river.datasets.CreditCard().take(8000), N_SAMPLES, seed=42)\\nj=0\\n#data_stream = stream.shuffle(make_credit_dataset(10,10000,df_test)[0].itertuples(index=False),N_SAMPLES, seed=SEED)\\ndata_stream = stream.shuffle(dataset[0].itertuples(index=False),N_SAMPLES, seed=SEED)\\ncounter_supervised=0\\nls=False\\nmodel3 = compose.Pipeline(\\n    preprocessing.MinMaxScaler(),\\n    anomaly.HalfSpaceTrees(seed=SEED)\\n)\\nfor x, y in data_stream:\\n    #model3.learn_one(x)\\n    #y_pred= model3.score_one(x)\\n#    if ls and y==0:\\n#        model3.learn_one(x)\\n    if not ls or counter_supervised==0:\\n        model3.learn_one(x)\\n        counter_supervised=counter_supervised+1\\n    y_pred= model3.score_one(x) #high score means outlier\\n    if j<5:\\n        print(y_pred)\\n        print(y)\\n        j=j+1\\n    rocauc.update(y,y_pred)\\n#evaluation.append(rocauc)\\nprint(rocauc)\\nevaluation=evaluation.append({\\n    'model':'HalfSpaceTrees_min_max_scaler',\\n    'anom_percentage_credit':anom_percentage_credit,\\n    'num_samples_credit':num_samples_credit,\\n    'Loss':LOSS,\\n    'Optimizer':OPTIMIZER,\\n    'Batch_size':BATCH_SIZE,\\n    'Learning_rate':LEARNING_RATE,\\n    'Latent_dim': LATENT_DIM,\\n    'ROC':rocauc,\\n    #'Precision_Recall_Curve':45,\\n    #'FP':5,\\n    #'TP': 4,\\n    #'FN':8,\\n    #'TN':10\\n\\n},ignore_index=True)\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model3 = compose.Pipeline(\n",
    "    preprocessing.MinMaxScaler(),\n",
    "    anomaly.HalfSpaceTrees(seed=SEED)\n",
    ")\n",
    "## gibt Anomalie Score aus\n",
    "\n",
    "rocauc= river.metrics.ROCAUC()\n",
    "#data_stream = stream.shuffle(river.datasets.CreditCard().take(8000), N_SAMPLES, seed=42)\n",
    "j=0\n",
    "#data_stream = stream.shuffle(make_credit_dataset(10,10000,df_test)[0].itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "data_stream = stream.shuffle(dataset[0].itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "counter_supervised=0\n",
    "ls=False\n",
    "model3 = compose.Pipeline(\n",
    "    preprocessing.MinMaxScaler(),\n",
    "    anomaly.HalfSpaceTrees(seed=SEED)\n",
    ")\n",
    "for x, y in data_stream:\n",
    "    #model3.learn_one(x)\n",
    "    #y_pred= model3.score_one(x)\n",
    "#    if ls and y==0:\n",
    "#        model3.learn_one(x)\n",
    "    if not ls or counter_supervised==0:\n",
    "        model3.learn_one(x)\n",
    "        counter_supervised=counter_supervised+1\n",
    "    y_pred= model3.score_one(x) #high score means outlier\n",
    "    if j<5:\n",
    "        print(y_pred)\n",
    "        print(y)\n",
    "        j=j+1\n",
    "    rocauc.update(y,y_pred)\n",
    "#evaluation.append(rocauc)\n",
    "print(rocauc)\n",
    "evaluation=evaluation.append({\n",
    "    'model':'HalfSpaceTrees_min_max_scaler',\n",
    "    'anom_percentage_credit':anom_percentage_credit,\n",
    "    'num_samples_credit':num_samples_credit,\n",
    "    'Loss':LOSS,\n",
    "    'Optimizer':OPTIMIZER,\n",
    "    'Batch_size':BATCH_SIZE,\n",
    "    'Learning_rate':LEARNING_RATE,\n",
    "    'Latent_dim': LATENT_DIM,\n",
    "    'ROC':rocauc,\n",
    "    #'Precision_Recall_Curve':45,\n",
    "    #'FP':5,\n",
    "    #'TP': 4,\n",
    "    #'FN':8,\n",
    "    #'TN':10\n",
    "\n",
    "},ignore_index=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDRegressor (SVM-like with Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata_stream = stream.shuffle(make_credit_dataset(10,5000,df_test)[0].itertuples(index=False),N_SAMPLES, seed=SEED)\\n\\n\\n#SGDRegressor (SVM-like with Kernel)\\nmodel6=compose.Pipeline(\\n    preprocessing.StandardScaler(),\\n    fx.RBFSampler(),\\n    compat.convert_sklearn_to_river(SGDClassifier(),classes=[False, True]),\\n    )\\n\\n#HoeffdingAdaptiveTreeClassifier\\nmodel8 = compose.Pipeline(\\n    preprocessing.StandardScaler(),\\n    HoeffdingAdaptiveTreeClassifier()\\n)\\n\\n#Logistic Regression\\nmodel9 = compose.Pipeline(\\n    preprocessing.StandardScaler(),\\n    linear_model.LogisticRegression(optimizer=op.SGD(0.1)))\\n\\nrocauc= river.metrics.ROCAUC(n_thresholds=100)\\n\\nfor x, y in data_stream:\\n    #y_pred= model3.predict_one(x)\\n    model3.learn_one(x,y)       \\n    rocauc.update(y,y_pred)\\n\\nrocauc\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "data_stream = stream.shuffle(make_credit_dataset(10,5000,df_test)[0].itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "\n",
    "\n",
    "#SGDRegressor (SVM-like with Kernel)\n",
    "model6=compose.Pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    fx.RBFSampler(),\n",
    "    compat.convert_sklearn_to_river(SGDClassifier(),classes=[False, True]),\n",
    "    )\n",
    "\n",
    "#HoeffdingAdaptiveTreeClassifier\n",
    "model8 = compose.Pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    HoeffdingAdaptiveTreeClassifier()\n",
    ")\n",
    "\n",
    "#Logistic Regression\n",
    "model9 = compose.Pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    linear_model.LogisticRegression(optimizer=op.SGD(0.1)))\n",
    "\n",
    "rocauc= river.metrics.ROCAUC(n_thresholds=100)\n",
    "\n",
    "for x, y in data_stream:\n",
    "    #y_pred= model3.predict_one(x)\n",
    "    model3.learn_one(x,y)       \n",
    "    rocauc.update(y,y_pred)\n",
    "\n",
    "rocauc\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HoeffdingAdaptiveTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"model8 = compose.Pipeline(\\n    preprocessing.StandardScaler(),\\n    HoeffdingAdaptiveTreeClassifier()\\n)\\n\\nrocauc= river.metrics.ROCAUC()\\n#data_stream = stream.shuffle(river.datasets.CreditCard().take(8000), N_SAMPLES, seed=42)\\ni=0\\nj=0\\ndata_stream = stream.shuffle(final_set.itertuples(index=False),N_SAMPLES, seed=SEED)\\nfor x, y in data_stream:\\n    #print(y)\\n    if i==0:\\n        model8.learn_one(x,y)\\n        i=i+1\\n    y_pred=model8.predict_one(x)\\n    #print(y_pred)\\n    model8.learn_one(x,y)\\n    rocauc.update(y,y_pred)\\n    if j<5:\\n        print(y_pred)\\n        print(y)\\n        j=j+1\\nrocauc\\nevaluation=evaluation.append({\\n    'model':'HalfSpaceTrees_min_max_scaler',\\n    'anom_percentage_credit':anom_percentage_credit,\\n    'num_samples_credit':num_samples_credit,\\n    'Loss':LOSS,\\n    'Optimizer':OPTIMIZER,\\n    'Batch_size':BATCH_SIZE,\\n    'Learning_rate':LEARNING_RATE,\\n    'Latent_dim': LATENT_DIM,\\n    'ROC':rocauc,\\n    #'Precision_Recall_Curve':45,\\n    #'FP':5,\\n    #'TP': 4,\\n    #'FN':8,\\n    #'TN':10\\n\\n},ignore_index=True)\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''model8 = compose.Pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    HoeffdingAdaptiveTreeClassifier()\n",
    ")\n",
    "\n",
    "rocauc= river.metrics.ROCAUC()\n",
    "#data_stream = stream.shuffle(river.datasets.CreditCard().take(8000), N_SAMPLES, seed=42)\n",
    "i=0\n",
    "j=0\n",
    "data_stream = stream.shuffle(final_set.itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "for x, y in data_stream:\n",
    "    #print(y)\n",
    "    if i==0:\n",
    "        model8.learn_one(x,y)\n",
    "        i=i+1\n",
    "    y_pred=model8.predict_one(x)\n",
    "    #print(y_pred)\n",
    "    model8.learn_one(x,y)\n",
    "    rocauc.update(y,y_pred)\n",
    "    if j<5:\n",
    "        print(y_pred)\n",
    "        print(y)\n",
    "        j=j+1\n",
    "rocauc\n",
    "evaluation=evaluation.append({\n",
    "    'model':'HalfSpaceTrees_min_max_scaler',\n",
    "    'anom_percentage_credit':anom_percentage_credit,\n",
    "    'num_samples_credit':num_samples_credit,\n",
    "    'Loss':LOSS,\n",
    "    'Optimizer':OPTIMIZER,\n",
    "    'Batch_size':BATCH_SIZE,\n",
    "    'Learning_rate':LEARNING_RATE,\n",
    "    'Latent_dim': LATENT_DIM,\n",
    "    'ROC':rocauc,\n",
    "    #'Precision_Recall_Curve':45,\n",
    "    #'FP':5,\n",
    "    #'TP': 4,\n",
    "    #'FN':8,\n",
    "    #'TN':10\n",
    "\n",
    "},ignore_index=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel9 = compose.Pipeline(\\n    preprocessing.StandardScaler(),\\n    linear_model.LogisticRegression(optimizer=op.SGD(0.1))\\n)\\n\\nrocauc= river.metrics.ROCAUC()\\ni=0\\n#data_stream = stream.shuffle(river.datasets.CreditCard().take(8000), N_SAMPLES, seed=42)\\ndata_stream = stream.shuffle(final_set.itertuples(index=False),N_SAMPLES, seed=SEED)\\nfor x, y in data_stream:\\n    y_pred=model9.predict_one(x)\\n    model9.learn_one(x,y)\\n    rocauc.update(y,y_pred)\\n    if i<5:\\n        print(y_pred)\\n        print(y)\\n        i=i+1\\n#evaluation.append(rocauc)\\nrocauc\\nevaluation=evaluation.append({\\n    'model':'HalfSpaceTrees_min_max_scaler',\\n    'anom_percentage_credit':anom_percentage_credit,\\n    'num_samples_credit':num_samples_credit,\\n    'Loss':LOSS,\\n    'Optimizer':OPTIMIZER,\\n    'Batch_size':BATCH_SIZE,\\n    'Learning_rate':LEARNING_RATE,\\n    'Latent_dim': LATENT_DIM,\\n    'ROC':rocauc,\\n    #'Precision_Recall_Curve':45,\\n    #'FP':5,\\n    #'TP': 4,\\n    #'FN':8,\\n    #'TN':10\\n\\n},ignore_index=True)\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model9 = compose.Pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    linear_model.LogisticRegression(optimizer=op.SGD(0.1))\n",
    ")\n",
    "\n",
    "rocauc= river.metrics.ROCAUC()\n",
    "i=0\n",
    "#data_stream = stream.shuffle(river.datasets.CreditCard().take(8000), N_SAMPLES, seed=42)\n",
    "data_stream = stream.shuffle(final_set.itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "for x, y in data_stream:\n",
    "    y_pred=model9.predict_one(x)\n",
    "    model9.learn_one(x,y)\n",
    "    rocauc.update(y,y_pred)\n",
    "    if i<5:\n",
    "        print(y_pred)\n",
    "        print(y)\n",
    "        i=i+1\n",
    "#evaluation.append(rocauc)\n",
    "rocauc\n",
    "evaluation=evaluation.append({\n",
    "    'model':'HalfSpaceTrees_min_max_scaler',\n",
    "    'anom_percentage_credit':anom_percentage_credit,\n",
    "    'num_samples_credit':num_samples_credit,\n",
    "    'Loss':LOSS,\n",
    "    'Optimizer':OPTIMIZER,\n",
    "    'Batch_size':BATCH_SIZE,\n",
    "    'Learning_rate':LEARNING_RATE,\n",
    "    'Latent_dim': LATENT_DIM,\n",
    "    'ROC':rocauc,\n",
    "    #'Precision_Recall_Curve':45,\n",
    "    #'FP':5,\n",
    "    #'TP': 4,\n",
    "    #'FN':8,\n",
    "    #'TN':10\n",
    "\n",
    "},ignore_index=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covtype Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 4 contains 9493 instances\n",
      "class 1 contains 283301 instances\n",
      "class 0 contains 211840 instances\n",
      "class 6 contains 20510 instances\n",
      "class 2 contains 35754 instances\n",
      "class 5 contains 17367 instances\n",
      "class 3 contains 2747 instances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-2a77468d0545>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  covtype_df.target[covtype_df.target==i]=1\n",
      "C:\\Users\\Manuel\\Anaconda3\\envs\\rwch\\lib\\site-packages\\pandas\\util\\_decorators.py:311: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream contains 12500 anomalies and 12500 no-anomalies\n",
      "0\n",
      "halfSpaceTrees_no2\n",
      "ROCAUC: 0.146988\n",
      "0\n",
      "halfSpaceTrees_no2\n",
      "ROCAUC: 0.458393\n",
      "class 4 contains 9493 instances\n",
      "class 1 contains 283301 instances\n",
      "class 0 contains 211840 instances\n",
      "class 6 contains 20510 instances\n",
      "class 2 contains 35754 instances\n",
      "class 5 contains 17367 instances\n",
      "class 3 contains 2747 instances\n",
      "Stream contains 12500 anomalies and 12500 no-anomalies\n",
      "0\n",
      "halfSpaceTrees_no2\n",
      "ROCAUC: 0.154865\n",
      "0\n",
      "halfSpaceTrees_no2\n",
      "ROCAUC: 0.526625\n",
      "class 4 contains 9493 instances\n",
      "class 1 contains 283301 instances\n",
      "class 0 contains 211840 instances\n",
      "class 6 contains 20510 instances\n",
      "class 2 contains 35754 instances\n",
      "class 5 contains 17367 instances\n",
      "class 3 contains 2747 instances\n",
      "Stream contains 12500 anomalies and 12500 no-anomalies\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-20220e3c1c93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     91\u001b[0m                         \u001b[1;31m#    print(y)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m                         \u001b[1;31m#    j=j+1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m                         \u001b[0mrocauc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmodel_counter\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mls\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_stream\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\rwch\\lib\\site-packages\\river\\metrics\\roc_auc.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mp_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthresholds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_true\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mbuild\\lib.win-amd64-3.7\\river\\metrics\\confusion.pyx\u001b[0m in \u001b[0;36mriver.metrics.confusion.ConfusionMatrix.update\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mbuild\\lib.win-amd64-3.7\\river\\metrics\\confusion.pyx\u001b[0m in \u001b[0;36mriver.metrics.confusion.ConfusionMatrix.majority_class.__get__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mbuild\\lib.win-amd64-3.7\\river\\metrics\\confusion.pyx\u001b[0m in \u001b[0;36mriver.metrics.confusion.ConfusionMatrix._majority_class\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margsort\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\rwch\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36margsort\u001b[1;34m(a, axis, kind, order)\u001b[0m\n\u001b[0;32m   1110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m     \"\"\"\n\u001b[1;32m-> 1112\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'argsort'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\rwch\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\rwch\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "counter=0\n",
    "counter_supervised=0\n",
    "#for i in range(0,anom_percentage_credit.__len__()):\n",
    "#    datasets_credit.append(make_credit_dataset(anom_percentage_credit[i],num_samples_credit,df_test))\n",
    "learn_supervised=[True,False]\n",
    "\n",
    "for i in range(0,anom_percentage_covtype.__len__()):\n",
    "    for j in range(0,num_anom_classes_covtype.__len__()):\n",
    "        dataset=make_covtype_dataset(anom_percentage_covtype[i],num_samples_covtype,num_anom_classes_covtype[j])\n",
    "        ls=False\n",
    "        for ls in learn_supervised:\n",
    "            stacked_ae_model = compose.Pipeline(\n",
    "            preprocessing.StandardScaler(),\n",
    "            TorchAE(\n",
    "                        build_fn = stacked_ae,\n",
    "                        loss_fn = LOSS,\n",
    "                        optimizer_fn = OPTIMIZER,\n",
    "                        learning_rate=LEARNING_RATE,\n",
    "                        seed=SEED\n",
    "                )\n",
    "            )\n",
    "            undercomplete_ae_model = compose.Pipeline(\n",
    "            preprocessing.StandardScaler(),\n",
    "            TorchAE(\n",
    "                        build_fn = undercomplete_ae,\n",
    "                        loss_fn = LOSS,\n",
    "                        optimizer_fn = OPTIMIZER,\n",
    "                        learning_rate=LEARNING_RATE,\n",
    "                        seed=SEED\n",
    "                    )\n",
    "                )\n",
    "            #HalfSpaceTrees\n",
    "\n",
    "            halfSpaceTrees = compose.Pipeline(\n",
    "                preprocessing.MinMaxScaler(),\n",
    "                anomaly.HalfSpaceTrees(seed=SEED)\n",
    "            )\n",
    "            #SGDRegressor (SVM-like with Kernel)\n",
    "            sgdregressor=compose.Pipeline(\n",
    "                preprocessing.StandardScaler(),\n",
    "                fx.RBFSampler(),\n",
    "                compat.convert_sklearn_to_river(SGDClassifier(),classes=[False, True]),\n",
    "                )\n",
    "            #HoeffdingAdaptiveTreeClassifier\n",
    "            hoeffdingAdaptiveTreeClassifier = compose.Pipeline(\n",
    "                preprocessing.StandardScaler(),\n",
    "                HoeffdingAdaptiveTreeClassifier()\n",
    "            )\n",
    "            #Logistic Regression\n",
    "            logistic_Regression = compose.Pipeline(\n",
    "                preprocessing.StandardScaler(),\n",
    "                linear_model.LogisticRegression(optimizer=op.SGD(0.1)))\n",
    "            #models = [undercomplete_ae_model,stacked_ae_model,halfSpaceTrees,sgdregressor,hoeffdingAdaptiveTreeClassifier,logistic_Regression]\n",
    "            #models = [undercomplete_ae_model,stacked_ae_model]\n",
    "            models = [halfSpaceTrees]\n",
    "            model_counter=0    \n",
    "            for model in models:          \n",
    "                counter_supervised=0\n",
    "                print(model_counter)\n",
    "                if model_counter==0:\n",
    "                    #model_name='undercomplete_ae_model'\n",
    "                    model_name='halfSpaceTrees_no2'\n",
    "                if model_counter==1:\n",
    "                    model_name='stacked_ae_model'\n",
    "                if model_counter==2:\n",
    "                    model_name='halfSpaceTrees'\n",
    "                if model_counter==3:\n",
    "                    model_name='sgdregressor'\n",
    "                if model_counter==4:\n",
    "                    model_name='hoeffdingAdaptiveTreeClassifier'\n",
    "                if model_counter==5:\n",
    "                    model_name='logistic_Regression'\n",
    "\n",
    "                rocauc= river.metrics.ROCAUC(n_thresholds=100)\n",
    "                data_stream = stream.shuffle(dataset[0].itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "                learning_counter=0\n",
    "                if model_counter<3:\n",
    "                    for x, y in data_stream:\n",
    "                        if learning_counter==0:\n",
    "                            model.learn_one(x)\n",
    "                            learning_counter=learning_counter+1\n",
    "                        y_pred= model.score_one(x) #high score means outlier\n",
    "                        if ls and y==0:\n",
    "                            model.learn_one(x)\n",
    "                        if not ls or counter_supervised==0:\n",
    "                            model.learn_one(x)\n",
    "                            counter_supervised=counter_supervised+1\n",
    "                        y_pred= model.score_one(x) #high score means outlier\n",
    "                        #if j<3:\n",
    "                        #    print(y_pred)\n",
    "                        #    print(y)\n",
    "                        #    j=j+1\n",
    "                        rocauc.update(y,y_pred)\n",
    "                if model_counter>2 and ls:\n",
    "                    for x, y in data_stream:\n",
    "                        if learning_counter==0:\n",
    "                            model.learn_one(x,y)\n",
    "                            learning_counter=learning_counter+1\n",
    "                        y_pred=model.predict_one(x)\n",
    "                        model.learn_one(x,y)\n",
    "                        rocauc.update(y,y_pred)\n",
    "\n",
    "                model_counter=model_counter+1                 \n",
    "                print(model_name)\n",
    "                print(rocauc)\n",
    "                evaluation=evaluation.append({\n",
    "                    'model': model_name,\n",
    "                    'num_neurons': [10,5,3],\n",
    "                    'dataset': dataset[1],\n",
    "                    'anom_percentage':dataset[2],\n",
    "                    'num_samples':dataset[3],\n",
    "                    'num_anom_classes':dataset[4],\n",
    "                    'learn_supervised':ls,\n",
    "                    'Loss':LOSS,\n",
    "                    'Optimizer':OPTIMIZER,\n",
    "                    'Batch_size':BATCH_SIZE,\n",
    "                    'Learning_rate':LEARNING_RATE,\n",
    "                    'Latent_dim': LATENT_DIM,\n",
    "                    'ROC':rocauc,\n",
    "                },ignore_index=True)\n",
    "\n",
    "#evaluation.to_csv('evaluation_8_halfSpaceTrees.vers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creditcard Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "counter_supervised=0\n",
    "#for i in range(0,anom_percentage_credit.__len__()):\n",
    "#    datasets_credit.append(make_credit_dataset(anom_percentage_credit[i],num_samples_credit,df_test))\n",
    "learn_supervised=[True,False]\n",
    "\n",
    "for i in range(0,anom_percentage_credit.__len__()):\n",
    "    for j in range(0,num_anom_classes_covtype.__len__()):\n",
    "        dataset=make_credit_dataset(anom_percentage_credit[i],num_samples_credit,df_test)\n",
    "        for ls in learn_supervised:\n",
    "            stacked_ae_model = compose.Pipeline(\n",
    "            preprocessing.StandardScaler(),\n",
    "            TorchAE(\n",
    "                        build_fn = stacked_ae,\n",
    "                        loss_fn = LOSS,\n",
    "                        optimizer_fn = OPTIMIZER,\n",
    "                        learning_rate=LEARNING_RATE,\n",
    "                        seed=SEED\n",
    "                )\n",
    "            )\n",
    "            undercomplete_ae_model = compose.Pipeline(\n",
    "            preprocessing.StandardScaler(),\n",
    "            TorchAE(\n",
    "                        build_fn = undercomplete_ae,\n",
    "                        loss_fn = LOSS,\n",
    "                        optimizer_fn = OPTIMIZER,\n",
    "                        learning_rate=LEARNING_RATE,\n",
    "                        seed=SEED\n",
    "                    )\n",
    "                )\n",
    "            halfSpaceTrees = compose.Pipeline(\n",
    "                preprocessing.MinMaxScaler(),\n",
    "                anomaly.HalfSpaceTrees(seed=SEED)\n",
    "            )\n",
    "            #SGDRegressor (SVM-like with Kernel)\n",
    "            sgdregressor=compose.Pipeline(\n",
    "                preprocessing.StandardScaler(),\n",
    "                fx.RBFSampler(),\n",
    "                compat.convert_sklearn_to_river(SGDClassifier(),classes=[False, True]),\n",
    "                )\n",
    "            #HoeffdingAdaptiveTreeClassifier\n",
    "            hoeffdingAdaptiveTreeClassifier = compose.Pipeline(\n",
    "                preprocessing.StandardScaler(),\n",
    "                HoeffdingAdaptiveTreeClassifier()\n",
    "            )\n",
    "            #Logistic Regression\n",
    "            logistic_Regression = compose.Pipeline(\n",
    "                preprocessing.StandardScaler(),\n",
    "                linear_model.LogisticRegression(optimizer=op.SGD(0.1)))\n",
    "            #models = [undercomplete_ae_model,stacked_ae_model,halfSpaceTrees,sgdregressor,hoeffdingAdaptiveTreeClassifier,logistic_Regression]\n",
    "            #models = [undercomplete_ae_model,stacked_ae_model]\n",
    "            models= [halfSpaceTrees]\n",
    "            model_counter=0    \n",
    "            for model in models:          \n",
    "                counter_supervised=0\n",
    "                print(model_counter)\n",
    "                if model_counter==0:\n",
    "                    model_name='undercomplete_ae_model'\n",
    "                if model_counter==1:\n",
    "                    model_name='stacked_ae_model'\n",
    "                if model_counter==2:\n",
    "                    model_name='halfSpaceTrees'\n",
    "                if model_counter==3:\n",
    "                    model_name='sgdregressor'\n",
    "                if model_counter==4:\n",
    "                    model_name='hoeffdingAdaptiveTreeClassifier'\n",
    "                if model_counter==5:\n",
    "                    model_name='logistic_Regression'\n",
    "\n",
    "                rocauc= river.metrics.ROCAUC(n_thresholds=100)\n",
    "                data_stream = stream.shuffle(dataset[0].itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "                learning_counter=0\n",
    "                counter1=0\n",
    "                counter2 =0                \n",
    "                if model_counter<3:\n",
    "                    for x, y in data_stream:\n",
    "                        if learning_counter==0:\n",
    "                            model.learn_one(x)\n",
    "                            learning_counter=learning_counter+1\n",
    "                        y_pred= model.score_one(x) #high score means outlier\n",
    "                        if ls and y==0:\n",
    "                            model.learn_one(x)\n",
    "                        if not ls or counter_supervised==0:\n",
    "                            model.learn_one(x)\n",
    "                            counter_supervised=counter_supervised+1\n",
    "                        y_pred= model.score_one(x) #high score means outlier\n",
    "\n",
    "                        if y==1:\n",
    "                            counter1=counter1+1\n",
    "                        if y==0:\n",
    "                            counter2=counter2+1\n",
    "                        #if j<3:\n",
    "                        #    print(y_pred)\n",
    "                        #    print(y)\n",
    "                        #    j=j+1\n",
    "                        rocauc.update(y,y_pred)\n",
    "                    print(counter1)\n",
    "                    print(counter2)\n",
    "\n",
    "                if model_counter>2 and ls:\n",
    "                    for x, y in data_stream:\n",
    "                        if learning_counter==0:\n",
    "                            model.learn_one(x,y)\n",
    "                            learning_counter=learning_counter+1\n",
    "                        y_pred=model.predict_one(x)\n",
    "                        model.learn_one(x,y)\n",
    "                        rocauc.update(y,y_pred)\n",
    "\n",
    "                model_counter=model_counter+1                 \n",
    "                print(model_name)\n",
    "                print(rocauc)\n",
    "                evaluation=evaluation.append({\n",
    "                    'model': model_name,\n",
    "                    'num_neurons': [10,5,3],\n",
    "                    'dataset': dataset[1],\n",
    "                    'anom_percentage':dataset[2],\n",
    "                    'num_samples':dataset[3],\n",
    "                    'num_anom_classes':dataset[4],\n",
    "                    'learn_supervised':ls,\n",
    "                    'Loss':LOSS,\n",
    "                    'Optimizer':OPTIMIZER,\n",
    "                    'Batch_size':BATCH_SIZE,\n",
    "                    'Learning_rate':LEARNING_RATE,\n",
    "                    'Latent_dim': LATENT_DIM,\n",
    "                    'ROC':rocauc,\n",
    "                },ignore_index=True)\n",
    "#evaluation.to_csv('evaluation_9_halfSpaceTrees.vers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random/Backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtergezogenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''undercomplete_ae_model = compose.Pipeline(\n",
    "    #preprocessing.StandardScaler(),\n",
    "    compat.PyTorch2RiverClassifier(\n",
    "                build_fn = undercomplete_ae,\n",
    "                loss_fn = nn.BCELoss,\n",
    "                optimizer_fn = OPTIMIZER,\n",
    "                #batch_size=BATCH_SIZE,\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                seed=SEED\n",
    "    )\n",
    ")\n",
    "'''\n",
    "'''undercomplete_ae_model = Autoencoder(\n",
    "                build_fn = undercomplete_ae,\n",
    "                loss_fn = LOSS,\n",
    "                optimizer_fn = OPTIMIZER,\n",
    "                #batch_size=BATCH_SIZE,\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                seed=SEED\n",
    "    )\n",
    "'''\n",
    "'''undercomplete_ae_model = compose.Pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    PyTorch2RiverClassifier(\n",
    "                build_fn = undercomplete_ae,\n",
    "                loss_fn = nn.MSELoss,\n",
    "                optimizer_fn = OPTIMIZER,\n",
    "                #batch_size=BATCH_SIZE,\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                seed=SEED\n",
    "    )\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD Undercomplete Autoencoder with Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def undercomplete_ae_sm(n_features, latent_dim=LATENT_DIM):\n",
    "    net = nn.Sequential(\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(n_features, 20), \n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(20, latent_dim),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(latent_dim, 20),\n",
    "        nn.LeakyReLU(), \n",
    "        nn.Linear(20, n_features),\n",
    "        nn.Linear(n_features,1),\n",
    "        nn.Softmax()\n",
    "    )\n",
    "    return net\n",
    "    '''\n",
    "\n",
    "'''undercomplete_ae_sm_model = compose.Pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    PyTorch2RiverClassifier(\n",
    "                build_fn = undercomplete_ae_sm,\n",
    "                loss_fn = LOSS,\n",
    "                optimizer_fn = OPTIMIZER,\n",
    "                #batch_size=BATCH_SIZE,\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                seed=SEED\n",
    "    )\n",
    ")\n",
    "'''\n",
    "\n",
    "'''#supervised learning approach with Softmax function --> Proba gets predicted, ROC way worse\n",
    "rocauc = river.metrics.ROCAUC()\n",
    "#data_stream = stream.shuffle(river.datasets.CreditCard().take(8000), N_SAMPLES, seed=42)\n",
    "data_stream = stream.shuffle(final_set.itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "for x, y in data_stream:\n",
    "    y_pred = undercomplete_ae_sm_model.predict_proba_one(x) #ruft learn_unsupervised auf, müssen wir learn_one dann überhaupt auch aufrufen?\n",
    "    undercomplete_ae_sm_model.learn_one(x, y) #undercomplete_ae_sm_model.learn_one ist supervised Ansatz, wir wollen Unsupervised\n",
    "    rocauc.update(y, y_pred)\n",
    "rocauc\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''## backup if anything fails\n",
    "rocauc= river.metrics.ROCAUC(n_thresholds=100)\n",
    "j=0\n",
    "learn_supervised=[True,False]\n",
    "models = [undercomplete_ae_model,stacked_ae_model]\n",
    "#learn_supervised=True\n",
    "y_pred_arr_undercomplete_ae = []\n",
    "y_act_arr_undercomplete_ae = []\n",
    "y_pred_cf=[]\n",
    "#f1 = river.metrics.F1()\n",
    "#cm=river.metrics.ConfusionMatrix()\n",
    "#data_stream = stream.shuffle(river.datasets.CreditCard().take(8000), N_SAMPLES, seed=SEED)\n",
    "for model in models:    \n",
    "    for ls in learn_supervised:\n",
    "        data_stream = stream.shuffle(final_set.itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "        for x, y in data_stream:\n",
    "            if ls and y==0:\n",
    "                undercomplete_ae_model.learn_one(x,y)\n",
    "            else:\n",
    "                undercomplete_ae_model.learn_one(x)\n",
    "            y_pred= undercomplete_ae_model.score_one(x) #high score means outlier\n",
    "            if j<5:\n",
    "                print(y_pred)\n",
    "                print(y)\n",
    "                j=j+1\n",
    "            rocauc.update(y,y_pred)\n",
    "        #    if y_pred>threshhold:\n",
    "        #        y_pred=1\n",
    "        #    else:\n",
    "        #        y_pred=0\n",
    "            y_pred_arr_undercomplete_ae.append([y_pred,y])\n",
    "            y_pred_cf.append(y_pred)\n",
    "            y_act_arr_undercomplete_ae.append(y)\n",
    "\n",
    "            #f1.update(y,y_pred)\n",
    "            #cm.update(y,y_pred)\n",
    "            #undercomplete_ae_model.learn_one(x,y_pred)\n",
    "            #undercomplete_ae_model.learn_one(x,learn_unsupervised=True)\n",
    "            #y_pred = undercomplete_ae_sm_model.predict_proba_one(x) #ruft learn_unsupervised auf, müssen wir learn_one dann überhaupt auch aufrufen?\n",
    "            #undercomplete_ae_model.learn_one(x, y)\n",
    "            #rocauc.update(y, y_pred)\n",
    "        print(rocauc)\n",
    "        evaluation=evaluation.append({\n",
    "            'model':'undercomplete_ae_model',\n",
    "            'anom_percentage_credit':anom_percentage_credit,\n",
    "            'num_samples_credit':num_samples_credit,\n",
    "            'Loss':LOSS,\n",
    "            'Optimizer':OPTIMIZER,\n",
    "            'Batch_size':BATCH_SIZE,\n",
    "            'Learning_rate':LEARNING_RATE,\n",
    "            'Latent_dim': LATENT_DIM,\n",
    "            'ROC':rocauc,\n",
    "            #'Precision_Recall_Curve':45,\n",
    "            #'FP':5,\n",
    "            #'TP': 4,\n",
    "            #'FN':8,\n",
    "            #'TN':10\n",
    "\n",
    "        },ignore_index=True)\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "43d093f43044a3a65e6980dc28089ab95a7d3d66b3284b0379db52f7d7b16821"
  },
  "kernelspec": {
   "display_name": "rwch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
