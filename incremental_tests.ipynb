{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "from torch import optim\n",
    "from river import compose, metrics, preprocessing, stream, anomaly, linear_model, datasets, compose\n",
    "from river import feature_extraction as fx\n",
    "\n",
    "\n",
    "from IncrementalTorch.anomaly.anomaly import Autoencoder, BasicAutoencoder\n",
    "from river import compat\n",
    "\n",
    "from tqdm import tqdm\n",
    "import river  \n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "from OnlineTorch.classifier import PyTorch2RiverClassifier\n",
    "from OnlineTorch.anomaly import TorchAE\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import SGDOneClassSVM\n",
    "from sklearn.cluster import k_means\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "N_SAMPLES = 1_000 #number of elements stored in memory\n",
    "SEED = 42\n",
    "#track_name = \"RBF\"\n",
    "#LOSS = nn.BCELoss\n",
    "LOSS = nn.L1Loss\n",
    "#LOSS = nn.CrossEntropyLoss\n",
    "OPTIMIZER = optim.AdamW\n",
    "BATCH_SIZE=10\n",
    "LEARNING_RATE=1e-3\n",
    "LATENT_DIM = 1\n",
    "rocauc = river.metrics.ROCAUC()\n",
    "rocauc=0\n",
    "\n",
    "threshhold=0\n",
    "\n",
    "##max 5% with 10000 samples\n",
    "anom_percentage = 10\n",
    "num_samples = 5000\n",
    "\n",
    "anom_percentage1 = 50\n",
    "num_samples1 = 25000\n",
    "num_anom_classes=1 #if you want 1 anom class type 1\n",
    "\n",
    "\n",
    "#F1,F2,F0,5 interessant?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.2+cpu'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation=pd.DataFrame(columns=['model','anom_percentage','num_samples','Loss','Optimizer','Batch_size','Learning_rate','Latent_dim','ROC','Precision_Recall_Curve','FP','TP','FN','TN'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass 5 contains 9493 instances\\nclass 2 contains 283301 instances\\nclass 1 contains 211840 instances\\nclass 7 contains 20510 instances\\nclass 3 contains 35754 instances\\nclass 6 contains 17367 instances\\nclass 4 contains 2747 instances\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "class 5 contains 9493 instances\n",
    "class 2 contains 283301 instances\n",
    "class 1 contains 211840 instances\n",
    "class 7 contains 20510 instances\n",
    "class 3 contains 35754 instances\n",
    "class 6 contains 17367 instances\n",
    "class 4 contains 2747 instances\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"evaluation.append({\\n    'model':1,\\n    'anom_percentage':anom_percentage,\\n    'num_samples':num_samples,\\n    'Loss':LOSS,\\n    'Optimizer':OPTIMIZER,\\n    'Batch_size':BATCH_SIZE,\\n    'Learning_rate':LEARNING_RATE,\\n    'Latent_dim': LATENT_DIM,\\n    'ROC':rocauc,\\n    #'Precision_Recall_Curve':45,\\n    #'FP':5,\\n    #'TP': 4,\\n    #'FN':8,\\n    #'TN':10\\n\\n},ignore_index=True)\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''evaluation.append({\n",
    "    'model':1,\n",
    "    'anom_percentage':anom_percentage,\n",
    "    'num_samples':num_samples,\n",
    "    'Loss':LOSS,\n",
    "    'Optimizer':OPTIMIZER,\n",
    "    'Batch_size':BATCH_SIZE,\n",
    "    'Learning_rate':LEARNING_RATE,\n",
    "    'Latent_dim': LATENT_DIM,\n",
    "    'ROC':rocauc,\n",
    "    #'Precision_Recall_Curve':45,\n",
    "    #'FP':5,\n",
    "    #'TP': 4,\n",
    "    #'FN':8,\n",
    "    #'TN':10\n",
    "\n",
    "},ignore_index=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covtype Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "covtype_x, covtype_y=fetch_covtype(as_frame=True,return_X_y=True)\n",
    "covtype_df=covtype_x\n",
    "covtype_df['target']=covtype_y\n",
    "covtype_df['target']=covtype_df['target']-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#covtype_df[covtype_df['target']==0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 4 contains 9493 instances\n",
      "class 1 contains 283301 instances\n",
      "class 0 contains 211840 instances\n",
      "class 6 contains 20510 instances\n",
      "class 2 contains 35754 instances\n",
      "class 5 contains 17367 instances\n",
      "class 3 contains 2747 instances\n"
     ]
    }
   ],
   "source": [
    "covtype_df_classes=covtype_df.target.unique()\n",
    "for i in covtype_df_classes:\n",
    "    print('class {0} contains {1} instances'.format(i,covtype_df[covtype_df.target==i].Elevation.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manuel\\AppData\\Local\\Temp/ipykernel_16200/2152947708.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  covtype_df.target[covtype_df.target==i]=1\n",
      "C:\\Users\\Manuel\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\util\\_decorators.py:311: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "for i in range (1,num_anom_classes+1):\n",
    "    covtype_df.target[covtype_df.target==i]=1\n",
    "    covtype_df.loc[covtype_df['target']==i, 'target']=1\n",
    "covtype_df_filtered=covtype_df[covtype_df.target<=num_anom_classes]\n",
    "covtype_df_filtered.dropna(inplace=True)\n",
    "covtype_df_filtered.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 4 contains 9493 instances\n",
      "class 1 contains 283301 instances\n",
      "class 0 contains 211840 instances\n",
      "class 6 contains 20510 instances\n",
      "class 2 contains 35754 instances\n",
      "class 5 contains 17367 instances\n",
      "class 3 contains 2747 instances\n"
     ]
    }
   ],
   "source": [
    "covtype_df_classes=covtype_df.target.unique()\n",
    "for i in covtype_df_classes:\n",
    "    print('class {0} contains {1} instances'.format(i,covtype_df[covtype_df.target==i].Elevation.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream contains 12500 anomalies and 12500 no-anomalies\n"
     ]
    }
   ],
   "source": [
    "num_anom = int(num_samples1*(anom_percentage1/100))\n",
    "num_clean = int(num_samples1-num_anom)\n",
    "\n",
    "anoms=covtype_df_filtered[covtype_df_filtered['target']==1].iloc[0:num_anom]\n",
    "clean = covtype_df_filtered[covtype_df_filtered['target']==0].iloc[0:num_clean]\n",
    "frames = [anoms,clean]\n",
    "print('Stream contains {} anomalies and {} no-anomalies'.format(anoms['target'].count(),clean['target'].count()))\n",
    "\n",
    "final_set = pd.concat(frames,ignore_index=True)\n",
    "\n",
    "\n",
    "data_stream1 = stream.shuffle(final_set.itertuples(index=False),N_SAMPLES, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_covtype=final_set.iloc[:,:-1].transpose().to_dict()\n",
    "y_covtype=final_set.iloc[:,-1].transpose().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x_covtype= pd.DataFrame.from_dict(x_covtype)\n",
    "df_y_covtype= pd.Series(y_covtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_test_test=pd.DataFrame(data=[x_covtype,y_covtype]).transpose()\n",
    "final_set=frames_test_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_set_x=final_set.iloc[:,:-1]\n",
    "final_set_y=final_set.iloc[:,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#covtype_df_filtered=covtype_df[covtype_df.target!=2].reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#covtype_df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit Card Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_stream = stream.shuffle(river.datasets.CreditCard().take(8000), N_SAMPLES, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_stream = stream.shuffle(river.datasets.Insects().take(8000), N_SAMPLES, seed=SEED)\n",
    "#for x, y in data_stream:\n",
    "#    print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream contains 492 anomalies and 4500 no-anomalies\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_anom = int(num_samples*(anom_percentage/100))\n",
    "num_clean = int(num_samples-num_anom)\n",
    "\n",
    "data_stream = stream.shuffle(river.datasets.CreditCard(), N_SAMPLES, seed=SEED)\n",
    "data1 = pd.DataFrame(data=data_stream)\n",
    "df_test= pd.DataFrame.from_dict(data1)\n",
    "\n",
    "anoms=df_test[df_test[1]==1].iloc[0:num_anom]\n",
    "clean = df_test[df_test[1]==0].iloc[0:num_clean]\n",
    "frames = [anoms,clean]\n",
    "print('Stream contains {} anomalies and {} no-anomalies'.format(anoms[0].count(),clean[0].count()))\n",
    "final_set = pd.concat(frames,ignore_index=True)\n",
    "\n",
    "\n",
    "data_stream1 = stream.shuffle(final_set.itertuples(index=False),N_SAMPLES, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>anom_percentage</th>\n",
       "      <th>num_samples</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Optimizer</th>\n",
       "      <th>Batch_size</th>\n",
       "      <th>Learning_rate</th>\n",
       "      <th>Latent_dim</th>\n",
       "      <th>ROC</th>\n",
       "      <th>Precision_Recall_Curve</th>\n",
       "      <th>FP</th>\n",
       "      <th>TP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [model, anom_percentage, num_samples, Loss, Optimizer, Batch_size, Learning_rate, Latent_dim, ROC, Precision_Recall_Curve, FP, TP, FN, TN]\n",
       "Index: []"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD Undercomplete Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def undercomplete_ae_sm(n_features, latent_dim=1):\\n    net = nn.Sequential(\\n        nn.Dropout(),\\n        nn.Linear(n_features, 20), \\n        nn.LeakyReLU(),\\n        nn.Linear(20, latent_dim),\\n        nn.LeakyReLU(),\\n        nn.Linear(latent_dim, 20),\\n        nn.LeakyReLU(), \\n        nn.Linear(20, n_features),\\n        nn.Linear(n_features,1),\\n        nn.Softmax()\\n    )\\n    return net\\n    '"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def undercomplete_ae_sm(n_features, latent_dim=1):\n",
    "    net = nn.Sequential(\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(n_features, 20), \n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(20, latent_dim),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(latent_dim, 20),\n",
    "        nn.LeakyReLU(), \n",
    "        nn.Linear(20, n_features),\n",
    "        nn.Linear(n_features,1),\n",
    "        nn.Softmax()\n",
    "    )\n",
    "    return net\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'undercomplete_ae_sm_model = compose.Pipeline(\\n    preprocessing.StandardScaler(),\\n    PyTorch2RiverClassifier(\\n                build_fn = undercomplete_ae_sm,\\n                loss_fn = LOSS,\\n                optimizer_fn = OPTIMIZER,\\n                #batch_size=BATCH_SIZE,\\n                learning_rate=LEARNING_RATE,\\n                seed=SEED\\n    )\\n)\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''undercomplete_ae_sm_model = compose.Pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    PyTorch2RiverClassifier(\n",
    "                build_fn = undercomplete_ae_sm,\n",
    "                loss_fn = LOSS,\n",
    "                optimizer_fn = OPTIMIZER,\n",
    "                #batch_size=BATCH_SIZE,\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                seed=SEED\n",
    "    )\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#supervised learning approach with Softmax function --> Proba gets predicted, ROC way worse\\nrocauc = river.metrics.ROCAUC()\\n#data_stream = stream.shuffle(river.datasets.CreditCard().take(8000), N_SAMPLES, seed=42)\\ndata_stream = stream.shuffle(final_set.itertuples(index=False),N_SAMPLES, seed=SEED)\\nfor x, y in data_stream:\\n    y_pred = undercomplete_ae_sm_model.predict_proba_one(x) #ruft learn_unsupervised auf, müssen wir learn_one dann überhaupt auch aufrufen?\\n    undercomplete_ae_sm_model.learn_one(x, y) #undercomplete_ae_sm_model.learn_one ist supervised Ansatz, wir wollen Unsupervised\\n    rocauc.update(y, y_pred)\\nrocauc\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#supervised learning approach with Softmax function --> Proba gets predicted, ROC way worse\n",
    "rocauc = river.metrics.ROCAUC()\n",
    "#data_stream = stream.shuffle(river.datasets.CreditCard().take(8000), N_SAMPLES, seed=42)\n",
    "data_stream = stream.shuffle(final_set.itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "for x, y in data_stream:\n",
    "    y_pred = undercomplete_ae_sm_model.predict_proba_one(x) #ruft learn_unsupervised auf, müssen wir learn_one dann überhaupt auch aufrufen?\n",
    "    undercomplete_ae_sm_model.learn_one(x, y) #undercomplete_ae_sm_model.learn_one ist supervised Ansatz, wir wollen Unsupervised\n",
    "    rocauc.update(y, y_pred)\n",
    "rocauc\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undercomplete Autoencoder standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undercomplete_ae(n_features, latent_dim=1):\n",
    "    net = nn.Sequential(\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(n_features, 20),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(20, latent_dim),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(latent_dim, 20),\n",
    "        nn.LeakyReLU(), \n",
    "        nn.Linear(20, n_features),\n",
    "#        nn.Linear(n_features, 1),\n",
    "#        nn.Sigmoid()\n",
    "    )\n",
    "    return net\n",
    "\n",
    "    #Sigmoid bringt nicht so viel, da immer angepasst auf das aktuelle Trainingsbeispiel --> ist nicht zwischen 0,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "undercomplete_ae_model = compose.Pipeline(\n",
    "    #preprocessing.StandardScaler(),\n",
    "    compat.PyTorch2RiverClassifier(\n",
    "                build_fn = undercomplete_ae,\n",
    "                loss_fn = nn.BCELoss,\n",
    "                optimizer_fn = OPTIMIZER,\n",
    "                #batch_size=BATCH_SIZE,\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                seed=SEED\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "undercomplete_ae_model = Autoencoder(\n",
    "                build_fn = undercomplete_ae,\n",
    "                loss_fn = LOSS,\n",
    "                optimizer_fn = OPTIMIZER,\n",
    "                #batch_size=BATCH_SIZE,\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                seed=SEED\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "undercomplete_ae_model = compose.Pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    TorchAE(\n",
    "                build_fn = undercomplete_ae,\n",
    "                loss_fn = LOSS,\n",
    "                optimizer_fn = OPTIMIZER,\n",
    "                #batch_size=BATCH_SIZE,\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                seed=SEED\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'undercomplete_ae_model = compose.Pipeline(\\n    preprocessing.StandardScaler(),\\n    PyTorch2RiverClassifier(\\n                build_fn = undercomplete_ae,\\n                loss_fn = nn.MSELoss,\\n                optimizer_fn = OPTIMIZER,\\n                #batch_size=BATCH_SIZE,\\n                learning_rate=LEARNING_RATE,\\n                seed=SEED\\n    )\\n)\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''undercomplete_ae_model = compose.Pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    PyTorch2RiverClassifier(\n",
    "                build_fn = undercomplete_ae,\n",
    "                loss_fn = nn.MSELoss,\n",
    "                optimizer_fn = OPTIMIZER,\n",
    "                #batch_size=BATCH_SIZE,\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                seed=SEED\n",
    "    )\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "rocauc= river.metrics.ROCAUC(n_thresholds=100)\n",
    "y_pred_arr_undercomplete_ae = []\n",
    "y_act_arr_undercomplete_ae = []\n",
    "y_pred_cf=[]\n",
    "#f1 = river.metrics.F1()\n",
    "#cm=river.metrics.ConfusionMatrix()\n",
    "#data_stream = stream.shuffle(river.datasets.CreditCard().take(8000), N_SAMPLES, seed=SEED)\n",
    "data_stream = stream.shuffle(final_set.itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "for x, y in data_stream:\n",
    "    undercomplete_ae_model.learn_one(x)\n",
    "    y_pred= undercomplete_ae_model.score_one(x)\n",
    "#    print(y_pred)\n",
    "    rocauc.update(y,y_pred)\n",
    "#    if y_pred>threshhold:\n",
    "#        y_pred=1\n",
    "#    else:\n",
    "#        y_pred=0\n",
    "    y_pred_arr_undercomplete_ae.append([y_pred,y])\n",
    "    y_pred_cf.append(y_pred)\n",
    "    y_act_arr_undercomplete_ae.append(y)\n",
    "\n",
    "    #f1.update(y,y_pred)\n",
    "    #cm.update(y,y_pred)\n",
    "    #undercomplete_ae_model.learn_one(x,y_pred)\n",
    "    #undercomplete_ae_model.learn_one(x,learn_unsupervised=True)\n",
    "    #y_pred = undercomplete_ae_sm_model.predict_proba_one(x) #ruft learn_unsupervised auf, müssen wir learn_one dann überhaupt auch aufrufen?\n",
    "    #undercomplete_ae_model.learn_one(x, y)\n",
    "    #rocauc.update(y, y_pred)\n",
    "rocauc\n",
    "evaluation=evaluation.append({\n",
    "    'model':'undercomplete_ae_model',\n",
    "    'anom_percentage':anom_percentage,\n",
    "    'num_samples':num_samples,\n",
    "    'Loss':LOSS,\n",
    "    'Optimizer':OPTIMIZER,\n",
    "    'Batch_size':BATCH_SIZE,\n",
    "    'Learning_rate':LEARNING_RATE,\n",
    "    'Latent_dim': LATENT_DIM,\n",
    "    'ROC':rocauc,\n",
    "    #'Precision_Recall_Curve':45,\n",
    "    #'FP':5,\n",
    "    #'TP': 4,\n",
    "    #'FN':8,\n",
    "    #'TN':10\n",
    "\n",
    "},ignore_index=True)\n",
    "#f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metric1 = metrics.F1()\n",
    "#metric1.revert(y_act_arr_undercomplete_ae,y_pred_cf)\n",
    "\n",
    "#metric1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred_arr_undercomplete_ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ROCAUC: 0.908965"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rocauc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "rocauc= river.metrics.ROCAUC(n_thresholds=100)\n",
    "y_pred_arr_undercomplete_ae = []\n",
    "y_act_arr_undercomplete_ae = []\n",
    "y_pred_cf=[]\n",
    "#f1 = river.metrics.F1()\n",
    "#cm=river.metrics.ConfusionMatrix()\n",
    "#data_stream = stream.shuffle(river.datasets.CreditCard().take(8000), N_SAMPLES, seed=SEED)\n",
    "data_stream = stream.shuffle(final_set.itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "for x, y in data_stream:\n",
    "    undercomplete_ae_model.learn_one(x)\n",
    "    y_pred= undercomplete_ae_model.score_one(x) #high score means outlier\n",
    "#    print(y_pred)\n",
    "    rocauc.update(y,y_pred)\n",
    "#    if y_pred>threshhold:\n",
    "#        y_pred=1\n",
    "#    else:\n",
    "#        y_pred=0\n",
    "    y_pred_arr_undercomplete_ae.append([y_pred,y])\n",
    "    y_pred_cf.append(y_pred)\n",
    "    y_act_arr_undercomplete_ae.append(y)\n",
    "\n",
    "    #f1.update(y,y_pred)\n",
    "    #cm.update(y,y_pred)\n",
    "    #undercomplete_ae_model.learn_one(x,y_pred)\n",
    "    #undercomplete_ae_model.learn_one(x,learn_unsupervised=True)\n",
    "    #y_pred = undercomplete_ae_sm_model.predict_proba_one(x) #ruft learn_unsupervised auf, müssen wir learn_one dann überhaupt auch aufrufen?\n",
    "    #undercomplete_ae_model.learn_one(x, y)\n",
    "    #rocauc.update(y, y_pred)\n",
    "rocauc\n",
    "evaluation=evaluation.append({\n",
    "    'model':'undercomplete_ae_model',\n",
    "    'anom_percentage':anom_percentage,\n",
    "    'num_samples':num_samples,\n",
    "    'Loss':LOSS,\n",
    "    'Optimizer':OPTIMIZER,\n",
    "    'Batch_size':BATCH_SIZE,\n",
    "    'Learning_rate':LEARNING_RATE,\n",
    "    'Latent_dim': LATENT_DIM,\n",
    "    'ROC':rocauc,\n",
    "    #'Precision_Recall_Curve':45,\n",
    "    #'FP':5,\n",
    "    #'TP': 4,\n",
    "    #'FN':8,\n",
    "    #'TN':10\n",
    "\n",
    "},ignore_index=True)\n",
    "#f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ROCAUC: 0.929231"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rocauc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred_arr_undercomplete_ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tes=pd.DataFrame(y_pred_arr_undercomplete_ae,columns=['y_pred','y_true'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_tes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_tes[df_tes['y_true']==1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tes=df_tes.sort_values(by=['y_pred'],ascending=False,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_tes.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_tes[df_tes['y_true']==1].sort_values(by='y_pred',ascending=True).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_ae(n_features, latent_dim=1):\n",
    "    net = nn.Sequential(\n",
    "        nn.Dropout(),\n",
    "        nn.Linear(n_features, 20),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(20, 15),\n",
    "        nn.LeakyReLU(),        \n",
    "        nn.Linear(15, 10),\n",
    "        nn.LeakyReLU(),        \n",
    "        nn.Linear(10, latent_dim),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(latent_dim, 10),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(10, 15),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(15, 20),\n",
    "        nn.LeakyReLU(),               \n",
    "        nn.Linear(20, n_features),\n",
    "    )\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_ae_model = compose.Pipeline(\n",
    "    preprocessing.MinMaxScaler(),\n",
    "    TorchAE(\n",
    "                build_fn = stacked_ae,\n",
    "                loss_fn = LOSS,\n",
    "                optimizer_fn = OPTIMIZER,\n",
    "                #batch_size=BATCH_SIZE,\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                seed=SEED\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "rocauc= river.metrics.ROCAUC()\n",
    "#data_stream = stream.shuffle(river.datasets.CreditCard().take(8000), N_SAMPLES, seed=42)\n",
    "data_stream = stream.shuffle(final_set.itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "for x, y in data_stream:\n",
    "    stacked_ae_model.learn_one(x)\n",
    "    y_pred= stacked_ae_model.score_one(x)\n",
    "    rocauc.update(y,y_pred)\n",
    "rocauc\n",
    "evaluation=evaluation.append({\n",
    "    'model':'undercomplete_stacked_ae_model',\n",
    "    'anom_percentage':anom_percentage,\n",
    "    'num_samples':num_samples,\n",
    "    'Loss':LOSS,\n",
    "    'Optimizer':OPTIMIZER,\n",
    "    'Batch_size':BATCH_SIZE,\n",
    "    'Learning_rate':LEARNING_RATE,\n",
    "    'Latent_dim': LATENT_DIM,\n",
    "    'ROC':rocauc,\n",
    "    #'Precision_Recall_Curve':45,\n",
    "    #'FP':5,\n",
    "    #'TP': 4,\n",
    "    #'FN':8,\n",
    "    #'TN':10\n",
    "\n",
    "},ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-AE-(Incremental Baselines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneClassSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.0'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "river.__version__  #0.9.0 required for OneClassSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''#funktioniert nur mit river=0.9, allerdings muss dann git repo geupdatet werden, da anomalie.anomaliedetector klasse nicht mehr in base sondern in anomly ist --> Wheels können nicht mehr so gebaut werden wie bisher\n",
    "model4 = compose.Pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    #fx.RBFSampler(),    \n",
    "    anomaly.QuantileThresholder(\n",
    "        anomaly.OneClassSVM(),\n",
    "        q=0.97 #q Anpassung viele Auswirkungen\n",
    "    )\n",
    ")\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ROCAUC: 0.543084"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rocauc= river.metrics.ROCAUC()\n",
    "data_stream = stream.shuffle(final_set.itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "for x, y in data_stream:\n",
    "    model4.learn_one(x)\n",
    "    y_pred= model4.score_one(x)\n",
    "    rocauc.update(y,y_pred)\n",
    "rocauc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HalfSpaceTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = compose.Pipeline(\n",
    "    preprocessing.MinMaxScaler(),\n",
    "    anomaly.HalfSpaceTrees(seed=SEED)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "rocauc= river.metrics.ROCAUC()\n",
    "#data_stream = stream.shuffle(river.datasets.CreditCard().take(8000), N_SAMPLES, seed=42)\n",
    "data_stream = stream.shuffle(final_set.itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "for x, y in data_stream:\n",
    "    model3.learn_one(x)\n",
    "    y_pred= model3.score_one(x)\n",
    "    #print(y_pred)\n",
    "    rocauc.update(y,y_pred)\n",
    "#evaluation.append(rocauc)\n",
    "rocauc\n",
    "evaluation=evaluation.append({\n",
    "    'model':'HalfSpaceTrees_min_max_scaler',\n",
    "    'anom_percentage':anom_percentage,\n",
    "    'num_samples':num_samples,\n",
    "    'Loss':LOSS,\n",
    "    'Optimizer':OPTIMIZER,\n",
    "    'Batch_size':BATCH_SIZE,\n",
    "    'Learning_rate':LEARNING_RATE,\n",
    "    'Latent_dim': LATENT_DIM,\n",
    "    'ROC':rocauc,\n",
    "    #'Precision_Recall_Curve':45,\n",
    "    #'FP':5,\n",
    "    #'TP': 4,\n",
    "    #'FN':8,\n",
    "    #'TN':10\n",
    "\n",
    "},ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ROCAUC: 0.732409"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rocauc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>anom_percentage</th>\n",
       "      <th>num_samples</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Optimizer</th>\n",
       "      <th>Batch_size</th>\n",
       "      <th>Learning_rate</th>\n",
       "      <th>Latent_dim</th>\n",
       "      <th>ROC</th>\n",
       "      <th>Precision_Recall_Curve</th>\n",
       "      <th>FP</th>\n",
       "      <th>TP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>undercomplete_ae_model</td>\n",
       "      <td>10</td>\n",
       "      <td>5000</td>\n",
       "      <td>&lt;class 'torch.nn.modules.loss.L1Loss'&gt;</td>\n",
       "      <td>&lt;class 'torch.optim.adamw.AdamW'&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>ROCAUC: 0.908965</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>undercomplete_ae_model</td>\n",
       "      <td>10</td>\n",
       "      <td>5000</td>\n",
       "      <td>&lt;class 'torch.nn.modules.loss.L1Loss'&gt;</td>\n",
       "      <td>&lt;class 'torch.optim.adamw.AdamW'&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>ROCAUC: 0.929231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>undercomplete_stacked_ae_model</td>\n",
       "      <td>10</td>\n",
       "      <td>5000</td>\n",
       "      <td>&lt;class 'torch.nn.modules.loss.L1Loss'&gt;</td>\n",
       "      <td>&lt;class 'torch.optim.adamw.AdamW'&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>ROCAUC: -0.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HalfSpaceTrees_min_max_scaler</td>\n",
       "      <td>10</td>\n",
       "      <td>5000</td>\n",
       "      <td>&lt;class 'torch.nn.modules.loss.L1Loss'&gt;</td>\n",
       "      <td>&lt;class 'torch.optim.adamw.AdamW'&gt;</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>ROCAUC: 0.732409</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            model anom_percentage num_samples  \\\n",
       "0          undercomplete_ae_model              10        5000   \n",
       "1          undercomplete_ae_model              10        5000   \n",
       "2  undercomplete_stacked_ae_model              10        5000   \n",
       "3   HalfSpaceTrees_min_max_scaler              10        5000   \n",
       "\n",
       "                                     Loss                          Optimizer  \\\n",
       "0  <class 'torch.nn.modules.loss.L1Loss'>  <class 'torch.optim.adamw.AdamW'>   \n",
       "1  <class 'torch.nn.modules.loss.L1Loss'>  <class 'torch.optim.adamw.AdamW'>   \n",
       "2  <class 'torch.nn.modules.loss.L1Loss'>  <class 'torch.optim.adamw.AdamW'>   \n",
       "3  <class 'torch.nn.modules.loss.L1Loss'>  <class 'torch.optim.adamw.AdamW'>   \n",
       "\n",
       "  Batch_size  Learning_rate Latent_dim               ROC  \\\n",
       "0         10          0.001          1  ROCAUC: 0.908965   \n",
       "1         10          0.001          1  ROCAUC: 0.929231   \n",
       "2         10          0.001          1       ROCAUC: -0.   \n",
       "3         10          0.001          1  ROCAUC: 0.732409   \n",
       "\n",
       "   Precision_Recall_Curve  FP  TP  FN  TN  \n",
       "0                     NaN NaN NaN NaN NaN  \n",
       "1                     NaN NaN NaN NaN NaN  \n",
       "2                     NaN NaN NaN NaN NaN  \n",
       "3                     NaN NaN NaN NaN NaN  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.to_csv('evaluation_1.vers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA (Incremental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manuel\\Anaconda3\\envs\\rwch\\lib\\site-packages\\sklearn\\decomposition\\_incremental_pca.py:337: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance = S ** 2 / (n_total_samples - 1)\n",
      "C:\\Users\\Manuel\\Anaconda3\\envs\\rwch\\lib\\site-packages\\sklearn\\decomposition\\_incremental_pca.py:338: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio = S ** 2 / np.sum(col_var * n_total_samples)\n",
      "C:\\Users\\Manuel\\Anaconda3\\envs\\rwch\\lib\\site-packages\\sklearn\\decomposition\\_incremental_pca.py:348: RuntimeWarning: Mean of empty slice.\n",
      "  self.noise_variance_ = explained_variance[self.n_components_ :].mean()\n",
      "C:\\Users\\Manuel\\AppData\\Roaming\\Python\\Python38\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ROCAUC: 0.499556"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipca = IncrementalPCA(n_components=LATENT_DIM)\n",
    "model6=compose.Pipeline(\n",
    "    preprocessing.MinMaxScaler(),\n",
    "    ipca #IncrementalPCA(n_components=LATENT_DIM, batch_size=1)\n",
    ")\n",
    "#model7=compose.Pipeline(\n",
    "#    preprocessing.MinMaxScaler(),\n",
    "#    SklearnAnomalyDetector(IncrementalPCA(n_components=10))\n",
    "#)\n",
    "#rocauc= river.metrics.ROCAUC()\n",
    "#data_stream = stream.shuffle(river.datasets.CreditCard().take(8000), N_SAMPLES, seed=SEED)\n",
    "#for x, y in data_stream:\n",
    "#    print(x.values())\n",
    "#data_stream = stream.shuffle(river.datasets.CreditCard().take(20000), N_SAMPLES, seed=SEED)\n",
    "data_stream = stream.shuffle(final_set.itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "#data_stream = list(data_stream)\n",
    "\n",
    "data1 = pd.DataFrame(data=data_stream)\n",
    "#data2 = pd.DataFrame.from_dict(data=data1,orient='index')\n",
    "#test=data1[0].to_dict()\n",
    "#df_test=pd.DataFrame.from_dict(test, orient='index')\n",
    "#df_test\n",
    "#pd.DataFrame.from_dict({(i,j): data1[i][j] \n",
    "#                           for i in data1.keys() \n",
    "#                           for j in data1[i].keys()},\n",
    "#                       orient='index')\n",
    "#data_stream = stream.shuffle(river.datasets.CreditCard().take(20000), N_SAMPLES, seed=SEED)\n",
    "data_stream = stream.shuffle(final_set.itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "data1 = pd.DataFrame(data=data_stream)\n",
    "df_test= pd.DataFrame.from_dict(data1)\n",
    "\n",
    "\n",
    "data_stream1 = stream.shuffle(df_test.itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "#data_stream = stream.shuffle(river.datasets.CreditCard().take(8000), N_SAMPLES, seed=SEED)\n",
    "data_stream = stream.shuffle(final_set.itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "#df=pd.read_csv(r'C:\\Users\\Manuel\\river_data\\CreditCard\\creditcard.csv')\n",
    "rocauc= river.metrics.ROCAUC()\n",
    "#data_stream = stream.shuffle(river.datasets.CreditCard().take(5000), N_SAMPLES, seed=40)\n",
    "MSE_score_arr = []\n",
    "values = []\n",
    "for x, y in data_stream:\n",
    "    #print(pd.DataFrame.from_dict(x))\n",
    "    ipca = ipca.partial_fit(pd.DataFrame.from_dict([x]))\n",
    "    x_trans = ipca.transform(pd.DataFrame.from_dict([x]))\n",
    "    inverse_trans= ipca.inverse_transform(x_trans)\n",
    "    MSE_score = ((np.array(pd.DataFrame.from_dict([x]))-np.array([inverse_trans]))**2).sum()\n",
    "    MSE_score_arr.append(MSE_score)\n",
    "    values.append(y)\n",
    "#    model7.learn_one(x)\n",
    "#    y_pred= model7.score_one(x)\n",
    "#    rocauc.update(y,y_pred)\n",
    "rocauc\n",
    "values_df = pd.Series(data=values)\n",
    "loss = pd.Series(data=MSE_score_arr)\n",
    "loss = (loss-np.min(loss))/(np.max(loss)-np.min(loss))\n",
    "loss.sum()\n",
    "rocauc = metrics.ROCAUC()\n",
    "for yt, yp in zip(values, loss):\n",
    "    #print(yt, yp)\n",
    "    rocauc = rocauc.update(yt, yp)\n",
    "rocauc\n",
    "#evaluation.append(rocauc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDRegressor (SVM-like with Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ROCAUC: 0.960653"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from river import evaluate\n",
    "x_arr=[]\n",
    "y_arr=[]\n",
    "y_pred_arr=[]\n",
    "\n",
    "data_stream = stream.shuffle(final_set.itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "\n",
    "\n",
    "model6=compose.Pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    fx.RBFSampler(),\n",
    "    compat.convert_sklearn_to_river(SGDClassifier(),classes=[False, True]),\n",
    "    )\n",
    "\n",
    "rocauc= river.metrics.ROCAUC(n_thresholds=100)\n",
    "\n",
    "for x, y in data_stream:\n",
    "    y_pred= model6.predict_one(x)\n",
    "    model6.learn_one(x,y)       \n",
    "    rocauc.update(y,y_pred)\n",
    "    x_arr.append([x])\n",
    "    y_arr.append(y)\n",
    "    y_pred_arr.append([y,y_pred])\n",
    "rocauc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test=pd.DataFrame(y_pred_arr,columns=['y_true','y_pred'])\n",
    "# test.sort_values(by=['y_true'], ascending=False).head(50)\n",
    "#test.sort_values(by=['y_pred'], ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HoeffdingAdaptiveTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.tree import HoeffdingAdaptiveTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model8 = compose.Pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    HoeffdingAdaptiveTreeClassifier()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "rocauc= river.metrics.ROCAUC()\n",
    "#data_stream = stream.shuffle(river.datasets.CreditCard().take(8000), N_SAMPLES, seed=42)\n",
    "i=0\n",
    "data_stream = stream.shuffle(final_set.itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "for x, y in data_stream:\n",
    "    #print(y)\n",
    "    if i==0:\n",
    "        model8.learn_one(x,y)\n",
    "        i=i+1\n",
    "    y_pred=model8.predict_one(x)\n",
    "    #print(y_pred)\n",
    "    model8.learn_one(x,y)\n",
    "    rocauc.update(y,y_pred)\n",
    "rocauc\n",
    "evaluation=evaluation.append({\n",
    "    'model':'HalfSpaceTrees_min_max_scaler',\n",
    "    'anom_percentage':anom_percentage,\n",
    "    'num_samples':num_samples,\n",
    "    'Loss':LOSS,\n",
    "    'Optimizer':OPTIMIZER,\n",
    "    'Batch_size':BATCH_SIZE,\n",
    "    'Learning_rate':LEARNING_RATE,\n",
    "    'Latent_dim': LATENT_DIM,\n",
    "    'ROC':rocauc,\n",
    "    #'Precision_Recall_Curve':45,\n",
    "    #'FP':5,\n",
    "    #'TP': 4,\n",
    "    #'FN':8,\n",
    "    #'TN':10\n",
    "\n",
    "},ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ROCAUC: 0.944648"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rocauc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import optim as op\n",
    "\n",
    "model9 = compose.Pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    linear_model.LogisticRegression(optimizer=op.SGD(0.1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "rocauc= river.metrics.ROCAUC()\n",
    "#data_stream = stream.shuffle(river.datasets.CreditCard().take(8000), N_SAMPLES, seed=42)\n",
    "data_stream = stream.shuffle(final_set.itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "for x, y in data_stream:\n",
    "    y_pred=model9.predict_one(x)\n",
    "    model9.learn_one(x,y)\n",
    "    rocauc.update(y,y_pred)\n",
    "#evaluation.append(rocauc)\n",
    "rocauc\n",
    "evaluation=evaluation.append({\n",
    "    'model':'HalfSpaceTrees_min_max_scaler',\n",
    "    'anom_percentage':anom_percentage,\n",
    "    'num_samples':num_samples,\n",
    "    'Loss':LOSS,\n",
    "    'Optimizer':OPTIMIZER,\n",
    "    'Batch_size':BATCH_SIZE,\n",
    "    'Learning_rate':LEARNING_RATE,\n",
    "    'Latent_dim': LATENT_DIM,\n",
    "    'ROC':rocauc,\n",
    "    #'Precision_Recall_Curve':45,\n",
    "    #'FP':5,\n",
    "    #'TP': 4,\n",
    "    #'FN':8,\n",
    "    #'TN':10\n",
    "\n",
    "},ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ROCAUC: 0.971211"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rocauc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ipca = IncrementalPCA(n_components=LATENT_DIM)\n",
    "model6=compose.Pipeline(\n",
    "    preprocessing.MinMaxScaler(),\n",
    "    ipca #IncrementalPCA(n_components=LATENT_DIM, batch_size=1)\n",
    ")\n",
    "#model7=compose.Pipeline(\n",
    "#    preprocessing.MinMaxScaler(),\n",
    "#    SklearnAnomalyDetector(IncrementalPCA(n_components=10))\n",
    "#)\n",
    "#rocauc= river.metrics.ROCAUC()\n",
    "#data_stream = stream.shuffle(river.datasets.CreditCard().take(8000), N_SAMPLES, seed=SEED)\n",
    "#for x, y in data_stream:\n",
    "#    print(x.values())\n",
    "#data_stream = stream.shuffle(river.datasets.CreditCard().take(20000), N_SAMPLES, seed=SEED)\n",
    "data_stream = stream.shuffle(final_set.itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "#data_stream = list(data_stream)\n",
    "\n",
    "data1 = pd.DataFrame(data=data_stream)\n",
    "#data2 = pd.DataFrame.from_dict(data=data1,orient='index')\n",
    "#test=data1[0].to_dict()\n",
    "#df_test=pd.DataFrame.from_dict(test, orient='index')\n",
    "#df_test\n",
    "#pd.DataFrame.from_dict({(i,j): data1[i][j] \n",
    "#                           for i in data1.keys() \n",
    "#                           for j in data1[i].keys()},\n",
    "#                       orient='index')\n",
    "#data_stream = stream.shuffle(river.datasets.CreditCard().take(20000), N_SAMPLES, seed=SEED)\n",
    "data_stream = stream.shuffle(final_set.itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "data1 = pd.DataFrame(data=data_stream)\n",
    "df_test= pd.DataFrame.from_dict(data1)\n",
    "\n",
    "\n",
    "data_stream1 = stream.shuffle(df_test.itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "#data_stream = stream.shuffle(river.datasets.CreditCard().take(8000), N_SAMPLES, seed=SEED)\n",
    "data_stream = stream.shuffle(final_set.itertuples(index=False),N_SAMPLES, seed=SEED)\n",
    "#df=pd.read_csv(r'C:\\Users\\Manuel\\river_data\\CreditCard\\creditcard.csv')\n",
    "rocauc= river.metrics.ROCAUC()\n",
    "#data_stream = stream.shuffle(river.datasets.CreditCard().take(5000), N_SAMPLES, seed=40)\n",
    "MSE_score_arr = []\n",
    "values = []\n",
    "for x, y in data_stream:\n",
    "    #print(pd.DataFrame.from_dict(x))\n",
    "    ipca = ipca.partial_fit(pd.DataFrame.from_dict([x]))\n",
    "    x_trans = ipca.transform(pd.DataFrame.from_dict([x]))\n",
    "    inverse_trans= ipca.inverse_transform(x_trans)\n",
    "    MSE_score = ((np.array(pd.DataFrame.from_dict([x]))-np.array([inverse_trans]))**2).sum()\n",
    "    MSE_score_arr.append(MSE_score)\n",
    "    values.append(y)\n",
    "#    model7.learn_one(x)\n",
    "#    y_pred= model7.score_one(x)\n",
    "#    rocauc.update(y,y_pred)\n",
    "rocauc\n",
    "values_df = pd.Series(data=values)\n",
    "loss = pd.Series(data=MSE_score_arr)\n",
    "loss = (loss-np.min(loss))/(np.max(loss)-np.min(loss))\n",
    "loss.sum()\n",
    "rocauc = metrics.ROCAUC()\n",
    "for yt, yp in zip(values, loss):\n",
    "    #print(yt, yp)\n",
    "    rocauc = rocauc.update(yt, yp)\n",
    "rocauc\n",
    "#evaluation.append(rocauc)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manuel\\Anaconda3\\envs\\rwch\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ROCAUC: 0.499488"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''dataset = datasets.synth.RandomRBF(seed_model=7, seed_sample=SEED,n_classes=2,n_features=200).take(N_SAMPLES)\n",
    "data_stream = stream.shuffle(dataset,buffer_size=N_SAMPLES)\n",
    "\n",
    "y_count=0\n",
    "y_0_count=0\n",
    "\n",
    "def build_fn(n_features):\n",
    "    net = nn.Sequential(\n",
    "        nn.Linear(n_features, 5),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(5, 5),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(5, 5),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(5, 5),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(5, 1),\n",
    "        nn.Softmax()\n",
    "    )\n",
    "    return net\n",
    "\n",
    "model = compose.Pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    PyTorch2RiverClassifier(\n",
    "                build_fn=build_fn,\n",
    "                loss_fn=nn.BCELoss,\n",
    "                optimizer_fn=optim.Adam,\n",
    "                batch_size=1,\n",
    "                learning_rate=1e-3,\n",
    "    )\n",
    ")\n",
    "\n",
    "for x, y in data_stream:\n",
    "    y_pred = model.predict_proba_one(x)\n",
    "    model.learn_one(x)\n",
    "    rocauc.update(y, y_pred)\n",
    "    #print(y)\n",
    "    if y==1:\n",
    "        y_count+=1\n",
    "    else:\n",
    "        y_0_count+=1\n",
    "rocauc'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "539"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_0_count"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "43d093f43044a3a65e6980dc28089ab95a7d3d66b3284b0379db52f7d7b16821"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('rwch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
